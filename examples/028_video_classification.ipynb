{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1439f55f",
   "metadata": {},
   "source": [
    "![Degirum banner](https://raw.githubusercontent.com/DeGirum/PySDKExamples/main/images/degirum_banner.png)\n",
    "## Video Classification with R3D_18\n",
    "\n",
    "This notebook is an example of how to run video classification with the **R3D_18 model**.  \n",
    "\n",
    "This notebook demonstrates how to use **DeGirum PySDK** to perform AI inference on video data using the **R3D_18 video classification model**. The model processes sequences of video frames to recognize and classify human activities.  \n",
    "\n",
    "In this notebook, we provide two workflows:  \n",
    "\n",
    "1. Running inference on a **video stream**.  \n",
    "2. Running inference with a **GStreamer pipeline** setup.  \n",
    "\n",
    "The results of the classification are displayed with timestamps and activity labels, making it easy to interpret detected actions within the video.  \n",
    "\n",
    "This script works with the following inference options:  \n",
    "\n",
    "- Run inference on **DeGirum Cloud Platform**  \n",
    "- Run inference on **DeGirum AI Server** deployed on a localhost or on some computer in your LAN or VPN  \n",
    "- Run inference on a **DeGirum Hailo accelerator** directly installed on your computer  \n",
    "\n",
    "To try different options, you need to specify the appropriate `hw_location` option.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c0e478",
   "metadata": {},
   "outputs": [],
   "source": [
    "!degirum token create"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6c8d0e",
   "metadata": {},
   "source": [
    "#### Specify where do you want to run your inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e093091f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hw_location: where you want to run inference\n",
    "#     \"@cloud\" to use DeGirum cloud\n",
    "#     \"@local\" to run on local machine\n",
    "#     IP address for AI server inference\n",
    "# model_zoo_url: url/path for model zoo\n",
    "#     cloud_zoo_url: valid for @cloud, @local, and ai server inference options\n",
    "#     '': ai server serving models from local folder\n",
    "#     path to json file: single model zoo in case of @local inference\n",
    "# lp_det_model_name: name of the model for license plate detection\n",
    "# lp_ocr_model_name: name of the model for license plate OCR\n",
    "# video_source: video source for inference\n",
    "#     camera index for local camera\n",
    "#     URL of RTSP stream\n",
    "#     URL of YouTube Video\n",
    "#     path to video file (mp4 etc)\n",
    "model_name = \"r3d_18--112x112_quant_hailort_multidevice_1\"\n",
    "inference_host_address = \"@local\"\n",
    "model_zoo_url = \"degirum/hailo\"\n",
    "\n",
    "# Specify the video you will run inference on\n",
    "video_source = \"../assets/archery.mp4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26778cfb",
   "metadata": {},
   "source": [
    "### Simple predictions on a video file\n",
    "\n",
    "To begin with, let’s take a look at how to run predictions on a video file using the **R3D_18 video classification model**.  \n",
    "For this, we define a simple function that reads frames from the video, prepares them into a fixed-length clip, and then runs inference with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66b619d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'category_id': 5, 'label': 'archery', 'score': 15.736677169799805}, {'category_id': 161, 'label': 'hurling (sport)', 'score': 12.822478294372559}, {'category_id': 141, 'label': 'golf chipping', 'score': 12.822478294372559}, {'category_id': 142, 'label': 'golf driving', 'score': 11.365378379821777}, {'category_id': 153, 'label': 'hitting baseball', 'score': 9.471148490905762}]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import degirum as dg\n",
    "\n",
    "# Load model\n",
    "model = dg.load_model(\n",
    "    model_name=model_name,\n",
    "    inference_host_address=inference_host_address,\n",
    "    zoo_url=model_zoo_url,\n",
    ")\n",
    "\n",
    "# Inference\n",
    "def infer_clip(path, T=16, size=112):\n",
    "    \"\"\"Run inference on a video clip using the R3D_18 model.\"\"\"\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "\n",
    "    # Read video frames\n",
    "    while True:\n",
    "        ok, frame = cap.read()\n",
    "        if not ok:\n",
    "            break\n",
    "        frame = cv2.cvtColor(cv2.resize(frame, (size, size)), cv2.COLOR_BGR2RGB)\n",
    "        frames.append(frame.astype(np.uint8))\n",
    "    cap.release()\n",
    "\n",
    "    if not frames:\n",
    "        return\n",
    "\n",
    "    # Pad or sample frames to fixed length T\n",
    "    if len(frames) < T:\n",
    "        frames += [frames[-1]] * (T - len(frames))\n",
    "    idxs = np.linspace(0, len(frames) - 1, T, dtype=int)\n",
    "    selected_frames = [frames[i] for i in idxs]\n",
    "\n",
    "    # Prepare input tensor\n",
    "    x = np.concatenate(selected_frames, axis=-1).reshape(1, size * size, 48).astype(np.uint8)\n",
    "\n",
    "    # Run inference\n",
    "    out = model(x)\n",
    "\n",
    "    # Handle results\n",
    "    if (\n",
    "        isinstance(out.results, list)\n",
    "        and out.results\n",
    "        and isinstance(out.results[0], dict)\n",
    "        and \"label\" in out.results[0]\n",
    "    ):\n",
    "        print(out.results[:5])\n",
    "    else:\n",
    "        scores = out.results[0][\"data\"].reshape(-1)\n",
    "        top5 = np.argsort(-scores)[:5]\n",
    "        labels = getattr(model, \"label_dictionary\", None)\n",
    "        if labels:\n",
    "            print([labels.get(int(i), str(int(i))) for i in top5])\n",
    "        else:\n",
    "            print(top5.tolist())\n",
    "\n",
    "\n",
    "# Run inference\n",
    "infer_clip(video_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798ed24d",
   "metadata": {},
   "source": [
    "#### Using Gstreamer using Pygobject module\n",
    "\n",
    "we will use pygobject to fetch frames using gstreamer inside our custom video generator. It will require you to install pygobject which is a python binding which provides bindings for gstreamer etc. To install pygobject, follow below steps:\n",
    "\n",
    "`sudo apt install python3-gi python3-gi-cairo`\n",
    "\n",
    "`pip install PyGObject`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d1162e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gi\n",
    "gi.require_version(\"Gst\", \"1.0\")\n",
    "from gi.repository import Gst\n",
    "import numpy as np\n",
    "import cv2\n",
    "import degirum as dg\n",
    "\n",
    "# Initialize GStreamer\n",
    "Gst.init(None)\n",
    "\n",
    "# Custom generator using PyGObject\n",
    "gstreamer_pipeline = (\n",
    "    \"v4l2src device=/dev/video0 ! \"\n",
    "    \"videoconvert ! video/x-raw,format=BGR ! \"\n",
    "    \"appsink name=sink emit-signals=true max-buffers=1 drop=true\"\n",
    ")\n",
    "\n",
    "# Clip / preprocess settings for R3D_18\n",
    "T_CLIP = 16       # number of frames per clip\n",
    "RESIZE_TO = 112   # model expects 112x112\n",
    "# The model expects packed FCR input as uint8 with shape (1, 112*112, 3*T)\n",
    "\n",
    "# Load Hailo model \n",
    "model = dg.load_model(\n",
    "    model_name=model_name,\n",
    "    inference_host_address=inference_host_address,\n",
    "    zoo_url=model_zoo_url\n",
    ")\n",
    "\n",
    "# Helper: FCR pack (preprocess)\n",
    "def pack_frames_to_fcr(frames, size=RESIZE_TO, T=T_CLIP):\n",
    "    \"\"\"\n",
    "    frames: list of T frames, each (size, size, 3), RGB, uint8\n",
    "    returns: (1, size*size, 3*T), uint8\n",
    "    \"\"\"\n",
    "    x = np.concatenate(frames, axis=-1)           # (H, W, 3*T)\n",
    "    x = x.reshape(1, size * size, 3 * T)          # (1, H*W, 3*T)\n",
    "    return x.astype(np.uint8)\n",
    "\n",
    "# Generator: yields model-ready clips\n",
    "#   - Pull frames from GStreamer\n",
    "#   - Preprocess: resize to 112x112, BGR->RGB, pack T frames\n",
    "def custom_video_generator(pipeline_str=gstreamer_pipeline, T=T_CLIP, size=RESIZE_TO):\n",
    "    pipeline = Gst.parse_launch(pipeline_str)\n",
    "    sink = pipeline.get_by_name(\"sink\")\n",
    "    if sink is None:\n",
    "        raise RuntimeError(\"appsink named 'sink' not found in pipeline.\")\n",
    "    pipeline.set_state(Gst.State.PLAYING)\n",
    "\n",
    "    try:\n",
    "        frames = []\n",
    "        while True:\n",
    "            sample = sink.emit(\"pull-sample\")\n",
    "            if sample is None:\n",
    "                break\n",
    "\n",
    "            # Extract BGR frame\n",
    "            buf = sample.get_buffer()\n",
    "            caps = sample.get_caps()\n",
    "            s = caps.get_structure(0)\n",
    "            width = s.get_value(\"width\")\n",
    "            height = s.get_value(\"height\")\n",
    "            ok, map_info = buf.map(Gst.MapFlags.READ)\n",
    "            if not ok:\n",
    "                continue\n",
    "            try:\n",
    "                frame_bgr = np.frombuffer(map_info.data, np.uint8).reshape((height, width, 3))\n",
    "            finally:\n",
    "                buf.unmap(map_info)\n",
    "\n",
    "            # --- Preprocess for R3D_18 ---\n",
    "            # 1) Resize to 112x112\n",
    "            # 2) Convert BGR -> RGB\n",
    "            frame_rgb112 = cv2.cvtColor(\n",
    "                cv2.resize(frame_bgr, (size, size)),\n",
    "                cv2.COLOR_BGR2RGB\n",
    "            ).astype(np.uint8)\n",
    "            frames.append(frame_rgb112)\n",
    "\n",
    "            # When we have T frames, pack and yield\n",
    "            if len(frames) == T:\n",
    "                x = pack_frames_to_fcr(frames, size=size, T=T)\n",
    "                yield x\n",
    "                frames.clear()\n",
    "    finally:\n",
    "        pipeline.set_state(Gst.State.NULL)\n",
    "\n",
    "# Run prediction and display\n",
    "try:\n",
    "    for result in model.predict_batch(custom_video_generator()):\n",
    "        # DeGirum provides an overlay image with labels/probabilities\n",
    "        cv2.imshow(\"Webcam Inference\", result.image_overlay)\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe943911",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "In this guide, we demonstrated how to use the **R3D_18 video classification model** with DeGirum PySDK for activity recognition in videos.  \n",
    "We covered two workflows:  \n",
    "\n",
    "1. Running predictions on a **video file** by building a simple frame generator that prepares clips of fixed length for the model.  \n",
    "2. Running predictions on a **live video stream** using a **GStreamer-based generator with PyGObject**, which allows direct access to camera or RTSP feeds.  \n",
    "\n",
    "Both approaches highlight the flexibility of DeGirum PySDK when working with video data. You can adapt the frame generator to apply custom preprocessing—such as resizing, cropping, or filtering—to better suit your application needs.  \n",
    "\n",
    "These methods enable you to seamlessly integrate video classification into real-world scenarios, whether you are working with offline recordings or real-time video streams. By leveraging DeGirum’s built-in overlays and multiple inference backends (Cloud, AI Server, or Hailo accelerator), you can quickly deploy and visualize accurate activity recognition results across a variety of environments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
