{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5584ba5a",
   "metadata": {},
   "source": [
    "# Monocular Depth Estimation\n",
    "\n",
    "This notebook demonstrates how to perform monocular depth estimation using DeGirum PySDK. Users will learn how to:\n",
    "\n",
    "* Load a monocular depth estimation model\n",
    "* Create a custom postprocessor to transform the results\n",
    "* Visualize the resulting depth map\n",
    "\n",
    "Simply uncomment a model of your choice and provide the necessary configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b557a44",
   "metadata": {},
   "source": [
    "## Configure and load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e79b17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import degirum as dg, degirum_tools as dgt\n",
    "\n",
    "# Choose inference host address\n",
    "inference_host_address = \"@cloud\"\n",
    "# inference_host_address = \"@local\"\n",
    "\n",
    "# Choose zoo url\n",
    "zoo_url = \"degirum/hailo\"\n",
    "# zoo_url = \"<path to local folder>\"\n",
    "\n",
    "# Choose model\n",
    "model_name = \"scdepthv3--256x320_quant_hailort_hailo8_1\"\n",
    "# model_name = \"fastdepth--224x224_quant_hailort_hailo8_1\"\n",
    "\n",
    "# Set token\n",
    "token = dgt.get_token()\n",
    "# token = ''  # leave empty for local inference\n",
    "\n",
    "model = dg.load_model(\n",
    "    model_name=model_name,\n",
    "    inference_host_address=inference_host_address,\n",
    "    zoo_url=zoo_url,\n",
    "    token=token\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ec5db8",
   "metadata": {},
   "source": [
    "## Create a custom postprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2368bb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, numpy as np\n",
    "\n",
    "\n",
    "def dequantize(result):\n",
    "    zero_pt = result['quantization']['zero']\n",
    "    scale = result['quantization']['scale']\n",
    "    \n",
    "    dq_data = (result['data'].astype(np.float32) - zero_pt) * scale\n",
    "\n",
    "    return dq_data\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "# All custom postprocessors inherit from dg.postprocessor.InferenceResults\n",
    "class DepthResults(dg.postprocessor.InferenceResults):\n",
    "    # Color map for visualization\n",
    "    # For more color maps see:\n",
    "    # https://docs.opencv.org/4.x/d3/d50/group__imgproc__colormap.html#ga9a805d8262bcbe273f16be9ea2055a65\n",
    "    color_map = cv2.COLORMAP_VIRIDIS\n",
    "    # Toggle for scdepth specific postprocessing\n",
    "    use_scdepth = False\n",
    "    # Toggle to automatically normalize results\n",
    "    # This ensures the resulting depth map values fall within [0.0, 1.0]\n",
    "    normalize_results = False\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)  # call base class constructor first\n",
    "\n",
    "        _resize_map = {\n",
    "            'nearest': cv2.INTER_NEAREST,\n",
    "            'bilinear': cv2.INTER_LINEAR,\n",
    "            'area': cv2.INTER_AREA,\n",
    "            'bicubic': cv2.INTER_CUBIC,\n",
    "            'lanczos': cv2.INTER_LANCZOS4\n",
    "        }\n",
    "\n",
    "        # Retrieve the raw tensor from the inference\n",
    "        # Raw tensors are elements in the list self._inference_results\n",
    "        # Depth models only return one tensor, so we reference the first element\n",
    "        data = self._inference_results[0]['data']\n",
    "    \n",
    "        # Dequantize if datatype is unsigned int or int\n",
    "        if (data.dtype.kind == 'u' or\n",
    "            data.dtype.kind == 'i'):\n",
    "            data = dequantize(self._inference_results[0])\n",
    "\n",
    "        data = data.squeeze(0)\n",
    "\n",
    "        # Reshape raw output to model input dimensions\n",
    "        # self._model_params contains the model JSON parameters\n",
    "        hwc_layout = self._model_params.InputTensorLayout[0] == 'NHWC'\n",
    "        if hwc_layout:\n",
    "            shape_idxs = (0, 1)\n",
    "        else:\n",
    "            shape_idxs = (1, 2)\n",
    "\n",
    "        if (data.shape[shape_idxs[0]] != self._model_params.InputH[0] or \n",
    "            data.shape[shape_idxs[1]] != self._model_params.InputW[0]):\n",
    "            data = np.reshape(data.squeeze(),\n",
    "                              (self._model_params.InputH[0], self._model_params.InputW[0], 1))\n",
    "        \n",
    "        # Fix tensor layout\n",
    "        if not hwc_layout:\n",
    "            data = np.transpose(data, (1, 2, 0))\n",
    "\n",
    "        # Resize the depth map back to the original image size\n",
    "        # self.image contains the raw (no preprocessing) input image\n",
    "        if self.image is not None:\n",
    "            resize_mode = _resize_map[self._model_params.InputResizeMethod[0]]\n",
    "            image_size = self.image.shape[:2][::-1]\n",
    "            data = cv2.resize(data, image_size, interpolation=resize_mode)\n",
    "            data = np.expand_dims(data, axis=0)\n",
    "        \n",
    "        if DepthResults.use_scdepth:\n",
    "            data = 1 / (sigmoid(data) * 10 + 0.009)  # hailo_model_zoo/core/postprocessing/depth_estimation_postprocessing.py\n",
    "\n",
    "        if DepthResults.normalize_results:\n",
    "            data = data.squeeze(0)\n",
    "            data = self.normalize_depth_map(data)\n",
    "            data = np.expand_dims(data, axis=0)\n",
    "\n",
    "        self._inference_results[0]['data'] = data\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_depth_map(depth_map):\n",
    "        # Linear transform from [min, max] to [0.0, 1.0]\n",
    "        return (depth_map - depth_map.min()) / (depth_map.max() - depth_map.min())\n",
    "    \n",
    "    # Function to convert a depth map into a RGB image.\n",
    "    def _convert_depth_to_image(self, depth_map):\n",
    "        depth_map = depth_map.squeeze(0)\n",
    "        if not DepthResults.normalize_results:\n",
    "            depth_map = self.normalize_depth_map(depth_map)\n",
    "        depth_map = (depth_map * 255).astype(np.uint8)\n",
    "        depth_map = cv2.applyColorMap(depth_map, DepthResults.color_map)\n",
    "        return depth_map\n",
    "    \n",
    "    # Visualization results are generated from this function.\n",
    "    @property\n",
    "    def image_overlay(self):\n",
    "        image = self._convert_depth_to_image(self._inference_results[0]['data'])\n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcdba90",
   "metadata": {},
   "source": [
    "## Perform inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2fe25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_source = \"../assets/dark_room.jpg\"\n",
    "\n",
    "# Set custom postprocessor\n",
    "if \"scdepth\" in model_name:\n",
    "    DepthResults.use_scdepth = True\n",
    "model.custom_postprocessor = DepthResults\n",
    "\n",
    "print(f\"Running inference using '{model_name}' on image source '{image_source}'\")\n",
    "inference_result = model(image_source)\n",
    "\n",
    "print(\"Press 'x' or 'q' to stop.\")\n",
    "\n",
    "# show results of inference\n",
    "with dgt.Display(\"AI Camera\") as output_display:\n",
    "    # Stack original image above image overlay.\n",
    "    depth_visuals = dgt.stack_images(\n",
    "        inference_result.image,\n",
    "        inference_result.image_overlay,\n",
    "        dimension=\"vertical\")\n",
    "    output_display.show_image(depth_visuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0ed3d2",
   "metadata": {},
   "source": [
    "## Retrieve relative depth value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3342a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_x = 128\n",
    "pixel_y = 256\n",
    "\n",
    "depth_map = inference_result.results[0]['data']\n",
    "\n",
    "if not DepthResults.normalize_results:\n",
    "    depth_map = DepthResults.normalize_depth_map(depth_map)\n",
    "    \n",
    "depth_val = depth_map[0, pixel_y, pixel_x]\n",
    "\n",
    "print(f\"The relative depth value at coordinates {pixel_x}, {pixel_y} is {depth_val}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dgenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
