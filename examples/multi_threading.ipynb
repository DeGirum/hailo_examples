{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multithreaded Video Stream Inference with DeGirum PySDK\n",
    "This script demonstrates how to run inference on multiple video streams (video files or webcams) simultaneously using DeGirum PySDK. It utilizes multithreading to independently process each stream with different AI models. Each thread runs inference on a specified video source or webcam and displays the results in a separate window. The script supports configurable inference settings, including cloud or local deployment, model zoo location, and token-based authentication. Simply update the configurations list to add or modify streams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import degirum as dg\n",
    "import degirum_tools\n",
    "\n",
    "# choose inference host address\n",
    "inference_host_address = \"@cloud\"\n",
    "# inference_host_address = \"@local\"\n",
    "\n",
    "# choose zoo_url\n",
    "zoo_url = \"degirum/models_hailort\"\n",
    "# zoo_url = \"../models\"\n",
    "\n",
    "# set token\n",
    "token = degirum_tools.get_token()\n",
    "# token = '' # leave empty for local inference\n",
    "\n",
    "# Define the configurations for video file and webcam\n",
    "configurations = [\n",
    "    {\n",
    "        \"model_name\": \"yolov8n_relu6_coco--640x640_quant_hailort_hailo8_1\",\n",
    "        \"source\": \"../assets/Traffic.mp4\",  # Video file\n",
    "        \"display_name\": \"Traffic Camera\"\n",
    "    },\n",
    "    {\n",
    "        \"model_name\": \"yolov8n_relu6_face--640x640_quant_hailort_hailo8_1\",\n",
    "        \"source\": 1,  # Webcam index\n",
    "        \"display_name\": \"Webcam Feed\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Function to run inference on a video stream (video file or webcam)\n",
    "def run_inference(model_name, source, inference_host_address, zoo_url, token, display_name):\n",
    "    # Load AI model\n",
    "    model = dg.load_model(\n",
    "        model_name=model_name,\n",
    "        inference_host_address=inference_host_address,\n",
    "        zoo_url=zoo_url,\n",
    "        token=token\n",
    "    )\n",
    "\n",
    "    with degirum_tools.Display(display_name) as output_display:\n",
    "        for inference_result in degirum_tools.predict_stream(model, source):\n",
    "            output_display.show(inference_result)\n",
    "    print(f\"Stream '{display_name}' has finished.\")\n",
    "\n",
    "\n",
    "# Create and start threads\n",
    "threads = []\n",
    "for config in configurations:\n",
    "    thread = threading.Thread(\n",
    "        target=run_inference,\n",
    "        args=(\n",
    "            config[\"model_name\"],\n",
    "            config[\"source\"],\n",
    "            inference_host_address,\n",
    "            zoo_url,\n",
    "            token,\n",
    "            config[\"display_name\"]\n",
    "        )\n",
    "    )\n",
    "    threads.append(thread)\n",
    "    thread.start()\n",
    "\n",
    "# Wait for all threads to finish\n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "\n",
    "print(\"All streams have been processed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "supervision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
