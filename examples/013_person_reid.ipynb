{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98d9b7ce",
   "metadata": {},
   "source": [
    "# Person Re-identification using PySDK\n",
    "This notebook demonstrates Person Re-Identification (Re-ID) using PySDK. Re-ID focuses on recognizing and matching people across different camera views based on their unique appearance, like clothing and body shape.\n",
    "\n",
    "The basic pipeline works like this:\n",
    "1. Detect people in the image using a person detection model.\n",
    "\n",
    "2. Crop each detected person using the bounding box coordinates.\n",
    "\n",
    "3. Apply the Person Re-ID model to the cropped images to extract the embeddings which can further be used to identify and match individuals across different images or camera views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802f0925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import degirum as dg, degirum_tools\n",
    "\n",
    "inference_host_address = \"@local\"\n",
    "zoo_url = \"degirum/hailo\"\n",
    "token = '' \n",
    "device_type = \"HAILORT/HAILO8L\"\n",
    "\n",
    "# Person detection model name \n",
    "person_det_model_name = \"yolov8n_relu6_person--640x640_quant_hailort_hailo8l_1\"\n",
    "\n",
    "# choose image source\n",
    "image_source = \"../assets/ThreePersons.jpg\"\n",
    "\n",
    "# load AI model\n",
    "person_det_model = dg.load_model(\n",
    "    model_name=person_det_model_name,\n",
    "    inference_host_address=inference_host_address,\n",
    "    zoo_url=zoo_url,\n",
    "    token=token,\n",
    "    device_type=device_type\n",
    "    \n",
    ")\n",
    "# Run the inference on the image and see results\n",
    "print(f\" Running inference using '{person_det_model_name}' on image source '{image_source}'\")\n",
    "person_detections = person_det_model(image_source)\n",
    "print(person_detections)\n",
    "\n",
    "print(\"Press 'x' or 'q' to stop.\")\n",
    "\n",
    "# show results of inference\n",
    "with degirum_tools.Display(\"AI Camera\") as output_display:\n",
    "    output_display.show_image(person_detections.image_overlay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d921e3b",
   "metadata": {},
   "source": [
    "### Visualizing Person crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a9acc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to visualize the crops\n",
    "import matplotlib.pyplot as plt\n",
    "def display_images(images, title=\"Images\", figsize=(15, 5)):\n",
    "    \"\"\"\n",
    "    Display a list of images in a single row using Matplotlib.\n",
    "\n",
    "    Parameters:\n",
    "    - images (list): List of images (NumPy arrays) to display.\n",
    "    - title (str): Title for the plot.\n",
    "    - figsize (tuple): Size of the figure.\n",
    "    \"\"\"\n",
    "    num_images = len(images)\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=figsize)\n",
    "    if num_images == 1:\n",
    "        axes = [axes]  # Make it iterable for a single image\n",
    "    for ax, image in zip(axes, images):\n",
    "        image_rgb = image[:, :, ::-1]  # Convert BGR to RGB\n",
    "        ax.imshow(image_rgb)\n",
    "        ax.axis('off')\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44443cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store the person crops\n",
    "crops = []\n",
    "\n",
    "# Process each detection result\n",
    "for person in person_detections.results:\n",
    "    # Extract bounding box (assumed in [x1, y1, x2, y2] format)\n",
    "    x1, y1, x2, y2 = map(int, person[\"bbox\"])  # Convert bbox coordinates to integers\n",
    "    person_crop = person_detections.image[y1:y2, x1:x2]  # Crop the person from the image\n",
    "    crops.append(person_crop)\n",
    "\n",
    "# Display person crops\n",
    "display_images(crops, title=\"Person Crops\", figsize=(5, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa5d79a",
   "metadata": {},
   "source": [
    "### Extracting embedding using a Person recognition model for each person crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64477094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the Person reid model name \n",
    "person_reid_model_name = \"osnet_x1_0_person_reid--256x128_quant_hailort_hailo8l_1\"\n",
    "# person_reid_model_name = \"repvgg_a0_person_reid--256x128_quant_hailort_hailo8l_1\" \n",
    "\n",
    "# load AI model\n",
    "person_reid_model = dg.load_model(\n",
    "    model_name=person_reid_model_name,\n",
    "    inference_host_address=inference_host_address,\n",
    "    zoo_url=zoo_url,\n",
    "    token=token,\n",
    "    device_type=device_type\n",
    "    \n",
    ")\n",
    "# Run the embedding model on each person crop to extract the embeddings of size 512\n",
    "for person_crop in crops:\n",
    "    person_embedding = person_reid_model(person_crop).results[0][\"data\"][0][0] # shape (1,512)\n",
    "    print (person_embedding.shape)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
