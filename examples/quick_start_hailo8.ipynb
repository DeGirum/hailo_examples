{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeGirum PySDK Tutorial for Hailo8\n",
    "In this notebook, we illustrate the main features of PySDK and how it can be used to quickly develop edge AI applications using the Hailo8 accelerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure degirum-tools package is installed\n",
    "!pip show degirum-tools || pip install degirum-tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic setup\n",
    "In this notebook, you can set the variable `inference_host_address` to three possible values:\n",
    "- `@local`: Inference runs on Hailo8 accelerator installed on your machine. \n",
    "- `@cloud`: Inference runs on Hailo8 accelerators hosted on DeGirum AI Hub. \n",
    "- IP address of AI server: Inference runs on an AI server on a machine with Hailo8 accelerator. If the AI server runs on the same machine as this code, you can specify the IP address as `localhost`\n",
    "\n",
    "DeGirum hosts a public model zoo `degirum/models_hailort` containing pre-compiled models optimized for Hailo8 devices. If working with an AI server or with a local Hailo8 device, there is no need to register for a DeGirum AI Hub account to access these models. However, if you do not have access to a Hailo8 accelerator but still want to test the models and code, you can use the hardware hosted by DeGirum on our AI Hub. To run inference on DeGirum AI hub, you need to have a token. Please see [this link](https://docs.degirum.com/content/hub/token/) on how to generate an access token. See this [github gist](https://gist.github.com/shashichilappagari/ab856f4ed85fbfb623bc949cf453925b) on how to store and set up the token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import degirum as dg, degirum_tools\n",
    "inference_host_address = \"@cloud\" # set to @local if you want to run inference on your local machine\n",
    "zoo_url = \"degirum/models_hailort\"\n",
    "token = degirum_tools.get_token() # paste your token here or leave it empty if running on local machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick Start\n",
    "DeGirum's PySDK provides simple APIs to run AI model inference. In general, there are three steps in running an AI model:\n",
    "1. Loading model\n",
    "2. Running inference on an input\n",
    "3. Visualizing inference results\n",
    "\n",
    "The code block below shows how to get started with PySDK. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import degirum and degirum_tools\n",
    "import degirum as dg, degirum_tools\n",
    "\n",
    "# set model name, inference host address, zoo url, token, and image source\n",
    "model_name = \"yolov8n_relu6_coco--640x640_quant_hailort_hailo8_1\"\n",
    "image_source='../assets/ThreePersons.jpg'\n",
    "\n",
    "# load AI model\n",
    "model = dg.load_model(\n",
    "    model_name=model_name,\n",
    "    inference_host_address=inference_host_address,\n",
    "    zoo_url=zoo_url,\n",
    "    token=token\n",
    ")\n",
    "\n",
    "# perform AI model inference on given image source\n",
    "print(f\" Running inference using '{model_name}' on image source '{image_source}'\")\n",
    "inference_result = model(image_source)\n",
    "\n",
    "# print('Inference Results \\n', inference_result)  # numeric results\n",
    "print(inference_result)\n",
    "print(\"Press 'x' or 'q' to stop.\")\n",
    "\n",
    "# show results of inference\n",
    "with degirum_tools.Display(\"AI Camera\") as output_display:\n",
    "    output_display.show_image(inference_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start with local model zoo\n",
    "In this repo, we provide a `models` folder with a couple of example models. You can use the code block as a reference to run models from a local folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import degirum as dg\n",
    "\n",
    "hailo_model_zoo = dg.connect(\n",
    "    inference_host_address='@local',\n",
    "    zoo_url='../models',    \n",
    ")\n",
    "\n",
    "print(hailo_model_zoo.list_models())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import degirum as dg, degirum_tools\n",
    "\n",
    "# set model name, inference host address, zoo url, token, and image source\n",
    "model_name = \"yolov8n_relu6_coco--640x640_quant_hailort_hailo8_1\"\n",
    "image_source='../assets/ThreePersons.jpg'\n",
    "\n",
    "# load AI model\n",
    "model = dg.load_model(\n",
    "    model_name=model_name,\n",
    "    inference_host_address=inference_host_address,\n",
    "    zoo_url='../models',\n",
    "    token=token\n",
    ")\n",
    "\n",
    "# perform AI model inference on given image source\n",
    "print(f\" Running inference using '{model_name}' on image source '{image_source}'\")\n",
    "inference_result = model(image_source)\n",
    "\n",
    "# print('Inference Results \\n', inference_result)  # numeric results\n",
    "print(inference_result)\n",
    "print(\"Press 'x' or 'q' to stop.\")\n",
    "\n",
    "# show results of inference\n",
    "with degirum_tools.Display(\"AI Camera\") as output_display:\n",
    "    output_display.show_image(inference_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numpy Array as `image_source`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "image_source='../assets/ThreePersons.jpg'\n",
    "image_array = cv2.imread(image_source)\n",
    "\n",
    "# perform AI model inference on given image source\n",
    "print(f\" Running inference using '{model_name}' on image array\")\n",
    "inference_result = model(image_array)\n",
    "\n",
    "# print('Inference Results \\n', inference_result)  # numeric results\n",
    "print(inference_result)\n",
    "print(\"Press 'x' or 'q' to stop.\")\n",
    "\n",
    "# show results of inference\n",
    "with degirum_tools.Display(\"AI Camera\") as output_display:\n",
    "    output_display.show_image(inference_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image URL as `image_source`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_source = \"https://raw.githubusercontent.com/DeGirum/PySDKExamples/main/images/ThreePersons.jpg\"\n",
    "\n",
    "# perform AI model inference on given image source\n",
    "print(f\" Running inference using '{model_name}' on image source '{image_source}'\")\n",
    "inference_result = model(image_source)\n",
    "\n",
    "# print('Inference Results \\n', inference_result)  # numeric results\n",
    "print(inference_result)\n",
    "print(\"Press 'x' or 'q' to stop.\")\n",
    "\n",
    "# show results of inference\n",
    "with degirum_tools.Display(\"AI Camera\") as output_display:\n",
    "    output_display.show_image(inference_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unified Output Visualization\n",
    "- The DeGirum PySDK provides a significant advantage with its **unified output visualization**, simplifying the process of understanding inference results across different model types.\n",
    "\n",
    "- The `display.show_image(inference_result)` function automatically adapts to the specific AI model type, whether:\n",
    "  - **Object Detection**: Overlays bounding boxes on detected objects.\n",
    "  - **Segmentation**: Highlights regions of interest with color-coded masks.\n",
    "  - **Classification**: Displays the predicted labels or class names.\n",
    "  - **Keypoints Detection**: Marks keypoints (e.g., facial landmarks or skeletal joints) on the image.\n",
    "\n",
    "- This unified approach eliminates the need for manual customization, providing a seamless and consistent visualization experience for various AI tasks.\n",
    "\n",
    "- The code block below demonstrates this advantage using a **widget-based interface**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display as ipy_display, clear_output as ipy_clear_output\n",
    "import degirum as dg\n",
    "import degirum_tools\n",
    "\n",
    "# Define models with categories\n",
    "model_categories = {\n",
    "    \"Classification\": {\n",
    "        \"model_name\": \"yolov8s_silu_imagenet--224x224_quant_hailort_hailo8_1\",\n",
    "    },\n",
    "    \"Detection\": {\n",
    "        \"model_name\": \"yolov8n_relu6_coco--640x640_quant_hailort_hailo8_1\",\n",
    "    },\n",
    "    \"Segmentation\": {\n",
    "        \"model_name\": \"yolov8n_silu_seg--640x640_quant_hailort_hailo8_1\",\n",
    "    },\n",
    "    \"Keypoints\": {\n",
    "        \"model_name\": \"yolov8n_relu6_widerface_kpts--640x640_quant_hailort_hailo8_1\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Widgets for category selection, image path input, and inference\n",
    "category_dropdown = widgets.Dropdown(\n",
    "    options=list(model_categories.keys()),\n",
    "    value=\"Classification\",\n",
    "    description=\"Category:\",\n",
    ")\n",
    "\n",
    "image_path_input = widgets.Text(\n",
    "    value=\"../assets/ThreePersons.jpg\",\n",
    "    description=\"Image Path:\",\n",
    "    placeholder=\"Enter the path to the image\",\n",
    ")\n",
    "\n",
    "run_button = widgets.Button(description=\"Run Inference\")\n",
    "output = widgets.Output()\n",
    "\n",
    "# Function to handle the button click\n",
    "def run_inference(change):\n",
    "    with output:\n",
    "        ipy_clear_output(wait=True)  # Clear previous outputs\n",
    "        try:\n",
    "            # Get selected category and input image path\n",
    "            category = category_dropdown.value\n",
    "            model_info = model_categories[category]\n",
    "            model_name = model_info[\"model_name\"]\n",
    "            image_source = image_path_input.value\n",
    "            \n",
    "            # Display selected category, model, and image path\n",
    "            print(f\"Category: {category}\")\n",
    "            print(f\"Model: {model_name}\")\n",
    "            print(f\"Image Source: {image_source}\")\n",
    "            \n",
    "            # Load the model (assuming `inference_host_address`, `zoo_url`, `token` are set)\n",
    "            print(\"Loading model...\")\n",
    "            model = dg.load_model(\n",
    "                model_name=model_name,\n",
    "                inference_host_address=inference_host_address,\n",
    "                zoo_url=zoo_url,\n",
    "                token=token,\n",
    "            )\n",
    "            \n",
    "            # Perform AI model inference\n",
    "            print(\"Running inference...\")\n",
    "            inference_result = model(image_source)\n",
    "            \n",
    "            # Display results\n",
    "            print(\"Inference Results:\", inference_result)\n",
    "            with degirum_tools.Display(\"AI Camera: Press q to exit\") as display:\n",
    "                display.show_image(inference_result)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Attach the function to the button click\n",
    "run_button.on_click(run_inference)\n",
    "\n",
    "# Display widgets and output\n",
    "ipy_display(category_dropdown, image_path_input, run_button, output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running Inference on Video Stream\n",
    "- The `predict_stream` function in `degirum_tools` provides a powerful and efficient way to perform AI inference on video streams in real-time. It processes video frames sequentially and returns inference results frame by frame, enabling seamless integration with various video input sources.\n",
    "- The code below shows how to use `predict_stream` on a video file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"yolov8n_relu6_coco--640x640_quant_hailort_hailo8_1\"\n",
    "video_source = '../assets/Traffic.mp4'\n",
    "\n",
    "# load AI model\n",
    "model = dg.load_model(\n",
    "    model_name=model_name,\n",
    "    inference_host_address=inference_host_address,\n",
    "    zoo_url=zoo_url,\n",
    "    token=token\n",
    ")\n",
    "\n",
    "\n",
    "with degirum_tools.Display(\"AI Camera\") as output_display:\n",
    "    for inference_result in degirum_tools.predict_stream(model, video_source):\n",
    "        output_display.show(inference_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support for various video sources\n",
    "- The `predict_stream` function works with different video sources such as webcams, RTSP streams, or video files.\n",
    "\n",
    "- This code below demonstrates how to run real-time AI inference using `predict_stream`, offering users an intuitive way to interact with DeGirum PySDK.\n",
    "\n",
    "- Key features of the interface include:\n",
    "  - **Model Selection**: Users can choose from a list of pre-configured models, such as object detection, segmentation, or keypoints detection.\n",
    "  - **Video Source Selection**: Users can select from available video sources like webcams, RTSP streams, or video files, with an additional option to specify custom paths or URLs.\n",
    "\n",
    "- The interface dynamically updates based on user inputs, ensuring seamless configuration of models and video sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define available models and video sources\n",
    "available_models = [\n",
    "    \"yolov8n_relu6_coco--640x640_quant_hailort_hailo8_1\",\n",
    "    \"yolov8n_relu6_face--640x640_quant_hailort_hailo8_1\",\n",
    "    \"yolov8n_relu6_widerface_kpts--640x640_quant_hailort_hailo8_1\",\n",
    "    \"yolov8n_silu_seg--640x640_quant_hailort_hailo8_1\",\n",
    "]\n",
    "\n",
    "video_sources = {\n",
    "    \"Webcam\": \"0\",  # Default webcam index as string to allow editing\n",
    "    \"RTSP Stream\": \"rtsp://your_rtsp_stream_address\",\n",
    "    \"Sample Video File\": \"../assets/Traffic.mp4\",\n",
    "}\n",
    "\n",
    "# Widgets for selecting model and video source\n",
    "model_dropdown = widgets.Dropdown(\n",
    "    options=available_models,\n",
    "    value=available_models[0],\n",
    "    description=\"Model:\",\n",
    ")\n",
    "\n",
    "video_source_dropdown = widgets.Dropdown(\n",
    "    options=list(video_sources.keys()),\n",
    "    value=\"Webcam\",\n",
    "    description=\"Source Type:\",\n",
    ")\n",
    "\n",
    "video_input_text = widgets.Text(\n",
    "    value=video_sources[\"Webcam\"],\n",
    "    description=\"Custom URL/Path:\",\n",
    "    placeholder=\"Enter webcam index, RTSP Stream, or file path\",\n",
    ")\n",
    "\n",
    "run_button = widgets.Button(description=\"Run Inference\")\n",
    "output = widgets.Output()\n",
    "\n",
    "# Function to handle dropdown change for video source\n",
    "def on_source_change(change):\n",
    "    new_value = change[\"new\"] if isinstance(change, dict) else change.new\n",
    "    if new_value in [\"Webcam\", \"RTSP Stream\", \"Sample Video File\"]:\n",
    "        video_input_text.disabled = False\n",
    "        video_input_text.value = video_sources[new_value]\n",
    "    else:\n",
    "        video_input_text.disabled = True\n",
    "        video_input_text.value = str(video_sources[new_value])\n",
    "\n",
    "# Attach the dropdown change event\n",
    "video_source_dropdown.observe(on_source_change, names=\"value\")\n",
    "\n",
    "# Function to handle the button click\n",
    "def run_inference(change):\n",
    "    with output:\n",
    "        ipy_clear_output(wait=True)  # Clear previous outputs\n",
    "        try:\n",
    "            # Get selected model and video source\n",
    "            selected_model = model_dropdown.value\n",
    "            source_type = video_source_dropdown.value\n",
    "            video_source = video_input_text.value\n",
    "            \n",
    "            # Convert video source to integer if it's the webcam\n",
    "            if source_type == \"Webcam\":\n",
    "                try:\n",
    "                    video_source = int(video_source)  # Ensure it's an integer\n",
    "                except ValueError:\n",
    "                    raise ValueError(\"Invalid webcam index. It must be an integer.\")\n",
    "            \n",
    "            # Display selected options\n",
    "            print(f\"Selected Model: {selected_model}\")\n",
    "            print(f\"Selected Source Type: {source_type}\")\n",
    "            print(f\"Video Source: {video_source}\")\n",
    "            \n",
    "            # Load the model\n",
    "            print(\"Loading model...\")\n",
    "            model = dg.load_model(\n",
    "                model_name=selected_model,\n",
    "                inference_host_address=inference_host_address,\n",
    "                zoo_url=zoo_url,\n",
    "                token=degirum_tools.get_token(),\n",
    "            )\n",
    "            \n",
    "            # Run AI inference on the video stream\n",
    "            print(\"Running inference on video stream. Press 'x' or 'q' to stop.\")            \n",
    "            \n",
    "            # Display inference results\n",
    "            with degirum_tools.Display(\"AI Camera\") as display:\n",
    "                for inference_result in degirum_tools.predict_stream(model, video_source):\n",
    "                    display.show(inference_result)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Attach the function to the button click\n",
    "run_button.on_click(run_inference)\n",
    "\n",
    "# Display widgets and output\n",
    "ipy_display(model_dropdown, video_source_dropdown, video_input_text, run_button, output)\n",
    "\n",
    "# Initialize the input field based on the default dropdown selection\n",
    "on_source_change({\"new\": \"Webcam\"})  # Passing a dictionary for manual initialization\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "supervision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
