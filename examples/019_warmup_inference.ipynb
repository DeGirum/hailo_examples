{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Running multiple models with warm-up\n",
        "\n",
        "This notebook demonstrates how to load and run multiple AI models using DeGirum PySDK on a Hailo-8 or Hailo-8L device.\n",
        "\n",
        "It showcases the model warm-up technique, which involves running a single dummy inference on each model after loading. This step ensures all runtime resources and tensor buffers are initialized, which avoids latency spikes during the first real inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install degirum degirum_tools numpy opencv-python -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Necessary imports and loading models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "6hJTRobOV-9C",
        "outputId": "bac9a248-1cf8-4c00-9852-a563f941ebbd"
      },
      "outputs": [],
      "source": [
        "import cv2, numpy as np, degirum as dg, degirum_tools, time\n",
        "from PIL import Image\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 1. SETUP\n",
        "# ------------------------------------------------------------------\n",
        "host = \"@local\"\n",
        "zoo = \"degirum/hailo\"\n",
        "device_type = \"HAILORT/HAILO8L\"\n",
        "token=''\n",
        "pose_model_name = \"yolov8n_relu6_coco_pose--640x640_quant_hailort_hailo8l_1\"\n",
        "face_model_name = \"scrfd_500m--640x640_quant_hailort_hailo8l_1\"\n",
        "face_vec_model_name = \"arcface_mobilefacenet--112x112_quant_hailort_hailo8l_1\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Comparing latency with warmup and without warmup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load a Hailo model\n",
        "model = dg.load_model(\n",
        "    model_name=pose_model_name,\n",
        "    inference_host_address=host,\n",
        "    token=token,\n",
        "    zoo_url=zoo,\n",
        "    device_type=device_type\n",
        ")\n",
        "\n",
        "dummy_input = np.zeros((640,640,3), dtype=np.uint8)\n",
        "\n",
        "# --- Inference WITHOUT warm-up ---\n",
        "start = time.time()\n",
        "_ = model(dummy_input)\n",
        "t1 = time.time() - start\n",
        "print(f\"First inference (no warm-up): {t1*1000:.1f} ms\")\n",
        "\n",
        "# --- Inference WITH warm-up ---\n",
        "_ = model(dummy_input)  # warm-up step\n",
        "\n",
        "start = time.time()\n",
        "_ = model(dummy_input)\n",
        "t2 = time.time() - start\n",
        "print(f\"Subsequent inference (warmed up): {t2*1000:.1f} ms\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multi-model inference pipeline with warmup\n",
        "\n",
        "\n",
        "We run Pose detection continuously on every frame. If a dummy condition is met (e.g., more than one person detected), we run face detection model to localize faces and then we use face embedding (vector) model on each detected face. This showcases how using a dummy inference reduces latency while model switching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Loading models and running warm-up inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "print(\"Loading models...\")\n",
        "PoseModel = dg.load_model(model_name=pose_model_name, inference_host_address=host, zoo_url=zoo, token=token, device_type=device_type)\n",
        "FaceModel = dg.load_model(model_name=face_model_name, inference_host_address=host, zoo_url=zoo, token=token, device_type=device_type)\n",
        "FaceVectorModel = dg.load_model(model_name=face_vec_model_name, inference_host_address=host, zoo_url=zoo, token=token, device_type=device_type)\n",
        "\n",
        "\n",
        "# Dummy image for warm-up\n",
        "dummy_pose_img = np.zeros((640,640,3), dtype=np.uint8)\n",
        "dummy_face_img = np.zeros((640,640,3), dtype=np.uint8)\n",
        "dummy_face_crop = np.zeros((112,112,3), dtype=np.uint8)\n",
        "\n",
        "print(\"Warming up models...\")\n",
        "PoseModel(dummy_pose_img)\n",
        "FaceModel(dummy_face_img)\n",
        "FaceVectorModel(dummy_face_crop)\n",
        "\n",
        "print(\"Warm-up complete. Models are ready for real-time inference.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Running inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "combined_model = degirum_tools.CombiningCompoundModel(PoseModel, FaceModel)\n",
        "inference_stream = degirum_tools.predict_stream(combined_model, 0)\n",
        "\n",
        "with degirum_tools.Display(\"Results\") as display:\n",
        "    for inference_result in inference_stream:\n",
        "        annotated_frame = inference_result.image_overlay.copy()  # copy to modify\n",
        "\n",
        "        for detection in inference_result.results:\n",
        "            if detection.get(\"label\") == \"face\" and \"bbox\" in detection:\n",
        "                print(\"\\n ------- POSE + FACE DETECTION RESULT -------\")\n",
        "                print(f\"- Detected: {detection['label']} at {detection.get('bbox', 'N/A')}\")\n",
        "                x1, y1, x2, y2 = map(int, detection[\"bbox\"])\n",
        "                face_crop = annotated_frame[y1:y2, x1:x2]\n",
        "\n",
        "                if face_crop.shape[0] > 0 and face_crop.shape[1] > 0:\n",
        "                    face_resized = cv2.resize(face_crop, (112, 112))\n",
        "                    vec_result = FaceVectorModel(face_resized)\n",
        "                    embedding = np.asarray(vec_result.results[0][\"data\"]).flatten()\n",
        "\n",
        "                    emb_id = embedding[0]\n",
        "                    emb_norm = np.linalg.norm(embedding)\n",
        "                    label_text = f\"VecID: {emb_id:.2f}, Norm: {emb_norm:.2f}\"\n",
        "\n",
        "                    # Print embedding debug info\n",
        "                    print(\"\\n -------- FACE VECTOR RESULT ----------\")\n",
        "                    print(f\"Embedding Length: {len(embedding)}\")\n",
        "                    print(f\"First 5 Vector Values: {embedding[:5]}\")\n",
        "                    print(f\"Norm: {emb_norm:.2f}\")\n",
        "\n",
        "                    # Draw label and box using OpenCV\n",
        "                    cv2.putText(annotated_frame, label_text, (x1, y1 - 10),\n",
        "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)\n",
        "                    cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (255, 255, 0), 1)\n",
        "\n",
        "        display.show(annotated_frame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
