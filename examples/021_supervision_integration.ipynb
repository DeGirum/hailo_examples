{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22a831c3",
   "metadata": {},
   "source": [
    "# Supervision Integration with DeGirum PySDK (Hailo8L/HAILO8)\n",
    "\n",
    "This notebook demonstrates how to visualize DeGirum PySDK inference outputs with the Supervision library—focusing purely on post-inference rendering (no server config here). It follows our modular guide style and uses a single `dg.load_model(...)` flow, with each example initializing its own image path.\n",
    "\n",
    "## What’s covered\n",
    "- **Object Detection**: boxes + labels (optional scores) using `BoxAnnotator` and `LabelAnnotator`\n",
    "- **Instance Segmentation**: masks + boxes + labels using `MaskAnnotator`, `BoxAnnotator`, `LabelAnnotator`\n",
    "- **Pose Estimation**: keypoints + skeleton edges using `VertexAnnotator` and `EdgeAnnotator`\n",
    "\n",
    "## What you’ll use\n",
    "- `degirum`, `degirum_tools` and `supervision` annotators\n",
    "- Lightweight adapters to return native Supervision objects:\n",
    "  - `degirum_to_sv_detections(...)` → `sv.Detections` + labels\n",
    "  - `degirum_to_sv_keypoints(...)` → `list[sv.KeyPoints]`\n",
    "\n",
    "## Tested with\n",
    "- Hailo8L / HAILO8 targets via DeGirum PySDK\n",
    "- `supervision==0.22.0`\n",
    "For additional visualization options, see the Supervision detection annotators documentation:\n",
    "https://supervision.roboflow.com/0.22.0/detection/annotators/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864ba625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import degirum as dg\n",
    "import degirum_tools\n",
    "import numpy as np\n",
    "import supervision as sv\n",
    "from supervision.detection.core import Detections\n",
    "from supervision.draw.color import Color\n",
    "from supervision.geometry.core import Position\n",
    "from supervision.annotators.core import LabelAnnotator, BoxAnnotator, MaskAnnotator\n",
    "\n",
    "# -------------------------------\n",
    "# 1) Detection / Segmentation\n",
    "# -------------------------------\n",
    "def degirum_to_sv_detections(results):\n",
    "    \"\"\"\n",
    "    Convert DeGirum detection/segmentation results to Supervision Detections + labels.\n",
    "\n",
    "    Input:\n",
    "      - `results`: list[dict] OR an object with `.results` (list[dict])\n",
    "        Each dict can include:\n",
    "          \"bbox\": [x1,y1,x2,y2], \"category_id\": int, \"label\": str, \"score\": float,\n",
    "          \"mask\": array-like (optional; bool or numeric)\n",
    "\n",
    "    Returns:\n",
    "      - dets: supervision.Detections (with .mask if present)\n",
    "      - labels: list[str] like \"class 0.97\"\n",
    "    \"\"\"\n",
    "    items = getattr(results, \"results\", results) or []\n",
    "    if not items:\n",
    "        return (\n",
    "            Detections(\n",
    "                xyxy=np.empty((0, 4), dtype=np.float32),\n",
    "                class_id=np.array([], dtype=int),\n",
    "                confidence=np.array([], dtype=np.float32),\n",
    "            ),\n",
    "            [],\n",
    "        )\n",
    "\n",
    "    bboxes, class_ids, confidences, labels = [], [], [], []\n",
    "    masks_collected = []\n",
    "\n",
    "    for det in items:\n",
    "        bboxes.append(det[\"bbox\"])\n",
    "        class_ids.append(det[\"category_id\"])\n",
    "        confidences.append(det[\"score\"])\n",
    "        labels.append(f'{det[\"label\"]} {det[\"score\"]:.2f}')\n",
    "\n",
    "        m = det.get(\"mask\", None)\n",
    "        if m is None:\n",
    "            masks_collected.append(None)\n",
    "            continue\n",
    "\n",
    "        m = np.asarray(m)\n",
    "        # Replace NaNs/Infs to avoid surprises\n",
    "        if np.issubdtype(m.dtype, np.floating):\n",
    "            m = np.nan_to_num(m, nan=0.0, posinf=1.0, neginf=0.0)\n",
    "\n",
    "        # Threshold to bool if needed\n",
    "        if m.dtype != bool:\n",
    "            mmax = float(m.max()) if m.size else 0.0\n",
    "            if mmax <= 1.0:\n",
    "                # Probability mask in [0,1] → adaptive threshold at half the observed max\n",
    "                thr = 0.5 * mmax\n",
    "            else:\n",
    "                # Assume 0..255 (or similar) → binary around the midpoint\n",
    "                thr = 128.0 if m.dtype == np.uint8 else 0.5 * mmax\n",
    "            m = m >= thr\n",
    "\n",
    "        # Handle singleton channel dims in either position\n",
    "        if m.ndim == 3 and 1 in m.shape:\n",
    "            m = np.squeeze(m)\n",
    "        if m.ndim != 2:\n",
    "            raise ValueError(f\"Mask must be 2D (H,W); got shape {m.shape}\")\n",
    "        masks_collected.append(m.astype(bool, copy=False))\n",
    "\n",
    "    xyxy = np.array(bboxes, dtype=np.float32)\n",
    "    cls = np.array(class_ids, dtype=int)\n",
    "    conf = np.array(confidences, dtype=np.float32)\n",
    "\n",
    "    if any(m is not None for m in masks_collected):\n",
    "        first_mask = next(m for m in masks_collected if m is not None)\n",
    "        H, W = first_mask.shape\n",
    "        masks = [(m if m is not None else np.zeros((H, W), dtype=bool)) for m in masks_collected]\n",
    "        dets = Detections(xyxy=xyxy, class_id=cls, confidence=conf, mask=np.stack(masks, axis=0))\n",
    "    else:\n",
    "        dets = Detections(xyxy=xyxy, class_id=cls, confidence=conf)\n",
    "\n",
    "    return dets, labels\n",
    "\n",
    "# -------------------------------\n",
    "# 2) Keypoints (Pose)\n",
    "# -------------------------------\n",
    "def degirum_to_sv_keypoints(results, *, min_kp_score: float = 0.3) -> list[sv.KeyPoints]:\n",
    "    \"\"\"\n",
    "    Convert DeGirum pose results (with 'landmarks') to a list of Supervision KeyPoints.\n",
    "\n",
    "    Input:\n",
    "      - `results`: list[dict] OR an object with `.results` (list[dict])\n",
    "        Each dict may include:\n",
    "          \"landmarks\": list of {\"landmark\": [x, y], \"score\": float?}\n",
    "\n",
    "    Returns:\n",
    "      - List[sv.KeyPoints], one per item that has landmarks (shape (1, K, 2))\n",
    "    \"\"\"\n",
    "    items = getattr(results, \"results\", results) or []\n",
    "    keypoints_list: list[sv.KeyPoints] = []\n",
    "\n",
    "    for det in items:\n",
    "        lms = det.get(\"landmarks\", None)\n",
    "        if not lms:\n",
    "            continue\n",
    "\n",
    "        coords = []\n",
    "        for lm in lms:\n",
    "            x, y = lm[\"landmark\"]\n",
    "            s = lm.get(\"score\", 1.0)\n",
    "            coords.append([float(x), float(y)] if s >= min_kp_score else [0.0, 0.0])\n",
    "\n",
    "        arr = np.array(coords, dtype=np.float32).reshape((1, -1, 2))  # (1, K, 2)\n",
    "        keypoints_list.append(sv.KeyPoints(xy=arr))\n",
    "\n",
    "    return keypoints_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749cb3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Connection settings ---\n",
    "inference_host_address = \"@local\"  # or None for cloud\n",
    "zoo_url = \"degirum/hailo\"          # or None for cloud\n",
    "token = ''  # cloud token if needed\n",
    "device_type = [\"HAILORT/HAILO8L\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511bc247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Object Detection Example (DeGirum → Supervision) ---\n",
    "\n",
    "image_detection = \"../assets/ThreePersons.jpg\"\n",
    "detection_model_name = \"yolov8n_relu6_face--640x640_quant_hailort_multidevice_1\"\n",
    "\n",
    "# Load model\n",
    "model_det = dg.load_model(\n",
    "    model_name=detection_model_name,\n",
    "    inference_host_address=inference_host_address,\n",
    "    zoo_url=zoo_url,\n",
    "    token=token,\n",
    "    device_type=device_type\n",
    ")\n",
    "print(f\"Loaded detection model: {detection_model_name}\")\n",
    "\n",
    "# Inference\n",
    "print(f\"Running detection on: {image_detection}\")\n",
    "res_det = model_det(image_detection)\n",
    "\n",
    "# NEW: Convert to Supervision objects (Detections + labels)\n",
    "det_dets, det_labels = degirum_to_sv_detections(res_det)\n",
    "\n",
    "# Annotate\n",
    "box_annotator = BoxAnnotator(color=Color(0, 255, 0), thickness=2)\n",
    "label_annotator = LabelAnnotator(color=Color(0, 0, 0), text_color=Color(255, 255, 255))\n",
    "\n",
    "img_det = res_det.image.copy()\n",
    "img_det = box_annotator.annotate(scene=img_det, detections=det_dets)\n",
    "img_det = label_annotator.annotate(scene=img_det, detections=det_dets, labels=det_labels)\n",
    "\n",
    "# Display\n",
    "print(\"Press 'x' or 'q' to stop.\")\n",
    "with degirum_tools.Display(\"Object Detection Example\") as output_display:\n",
    "    output_display.show_image(img_det)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e5027c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Instance Segmentation (clean, single-pass annotators) ---\n",
    "\n",
    "image_segmentation = \"../assets/ThreePersons.jpg\"\n",
    "seg_model_name = \"yolov8n_coco_seg--640x640_quant_hailort_multidevice_1\"\n",
    "\n",
    "# Load model\n",
    "model_seg = dg.load_model(\n",
    "    model_name=seg_model_name,\n",
    "    inference_host_address=inference_host_address,\n",
    "    zoo_url=zoo_url,\n",
    "    token=token,\n",
    "    device_type=device_type\n",
    ")\n",
    "print(f\"Loaded segmentation model: {seg_model_name}\")\n",
    "\n",
    "# Inference\n",
    "print(f\"Running segmentation on: {image_segmentation}\")\n",
    "res_seg = model_seg(image_segmentation)\n",
    "\n",
    "# Convert to Supervision objects (Detections + labels; includes .mask if present)\n",
    "seg_dets, seg_labels = degirum_to_sv_detections(res_seg)\n",
    "\n",
    "# Annotators\n",
    "mask_annotator  = sv.MaskAnnotator()  # uses default palette\n",
    "box_annotator   = sv.BoxAnnotator(color=Color(0, 255, 0), thickness=2)\n",
    "label_annotator = sv.LabelAnnotator(\n",
    "    color=Color(0, 0, 0),\n",
    "    text_color=Color(255, 255, 255),\n",
    "    text_position=Position.TOP_LEFT,\n",
    "    text_scale=0.5, text_thickness=1, text_padding=4, border_radius=2\n",
    ")\n",
    "\n",
    "# Annotate in-place, single pass per annotator\n",
    "img_seg = res_seg.image.copy()\n",
    "img_seg = mask_annotator.annotate(img_seg, seg_dets)                     # masks\n",
    "img_seg = box_annotator.annotate(img_seg, seg_dets)                      # boxes\n",
    "img_seg = label_annotator.annotate(img_seg, seg_dets, labels=seg_labels) # labels\n",
    "\n",
    "# Display\n",
    "print(\"Press 'x' or 'q' to stop.\")\n",
    "with degirum_tools.Display(\"Instance Segmentation Example\") as output_display:\n",
    "    output_display.show_image(img_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2050429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pose Estimation Example (DeGirum → Supervision) ---\n",
    "\n",
    "image_pose = \"../assets/ThreePersons.jpg\"\n",
    "pose_model_name = \"yolov8n_relu6_coco_pose--640x640_quant_hailort_hailo8l_1\"\n",
    "\n",
    "# Load model\n",
    "model_pose = dg.load_model(\n",
    "    model_name=pose_model_name,\n",
    "    inference_host_address=inference_host_address,\n",
    "    zoo_url=zoo_url,\n",
    "    token=token,\n",
    "    device_type=device_type\n",
    ")\n",
    "print(f\"Loaded pose model: {pose_model_name}\")\n",
    "\n",
    "# Inference\n",
    "print(f\"Running pose estimation on: {image_pose}\")\n",
    "res_pose = model_pose(image_pose)\n",
    "img_pose = res_pose.image.copy()\n",
    "\n",
    "# NEW: Convert to Supervision KeyPoints (one per person)\n",
    "keypoints_list = degirum_to_sv_keypoints(res_pose, min_kp_score=0.3)\n",
    "\n",
    "# COCO-style edges for skeleton\n",
    "COCO_EDGES = [\n",
    "    (6, 7), (6, 8),      # shoulders to elbows\n",
    "    (8, 10), (7, 9),     # elbows to wrists\n",
    "    (6, 12), (7, 13),    # shoulders to hips\n",
    "    (12, 14), (14, 16),  # left leg\n",
    "    (13, 15), (15, 17),  # right leg\n",
    "    (12, 13), (9, 11)    # hips and left wrist–hip link (custom)\n",
    "]\n",
    "\n",
    "# Annotators\n",
    "vertex_annotator = sv.VertexAnnotator(color=Color(0, 255, 0), radius=5)\n",
    "edge_annotator   = sv.EdgeAnnotator(color=Color(255, 0, 0), thickness=2, edges=COCO_EDGES)\n",
    "\n",
    "# Draw skeletons\n",
    "if keypoints_list:\n",
    "    for kpts in keypoints_list:\n",
    "        img_pose = edge_annotator.annotate(scene=img_pose, key_points=kpts)\n",
    "        img_pose = vertex_annotator.annotate(scene=img_pose, key_points=kpts)\n",
    "else:\n",
    "    print(\"No keypoints found.\")\n",
    "\n",
    "# Display\n",
    "print(\"Press 'x' or 'q' to stop.\")\n",
    "with degirum_tools.Display(\"Pose Estimation Example\") as output_display:\n",
    "    output_display.show_image(img_pose)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
