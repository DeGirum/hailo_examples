{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b3324b0",
   "metadata": {},
   "source": [
    "## Understanding Eager Batching on Hailo devices with DeGirum PySDK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94934cdd",
   "metadata": {},
   "source": [
    "This guide demonstrates how **eager batching** affects inference performance on Hailo-8 and Hailo-8L devices using the DeGirum PySDK. It walks through live benchmarks comparing different models and batching strategies. **Eager batching** in DeGirum PySDK queues multiple inference requests and sends them to the accelerator as a batch. This can improve inference throughput—**but only for multi-context models**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458aa4e0",
   "metadata": {},
   "source": [
    "#### Understanding Context Size\n",
    "\n",
    "In Hailo's architecture, **context size** refers to how many model instances (contexts) can be concurrently loaded into the accelerator:\n",
    "\n",
    "- **Single-context models**: Fully fit into Hailo's memory. Adding batching doesn’t improve performance because the model already saturates the accelerator.\n",
    "  \n",
    "- **Multi-context models**: Too large to fit completely, so Hailo divides execution across contexts. Batching allows better hardware utilization and **can significantly improve FPS**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dbaafd",
   "metadata": {},
   "source": [
    "## What This Guide Demonstrates\n",
    "\n",
    "We run two models:\n",
    "- A **YOLOv8n object detection model** (context = 1)\n",
    "- A **YOLOv8m object detection model** (context > 1)\n",
    "\n",
    "Each is evaluated under:\n",
    "- `eager_batch_size = 1` (no batching)\n",
    "- `eager_batch_size = 8` (batched)\n",
    "\n",
    "We measure **frames per second (FPS)** using PySDK and visualize the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b38e5c",
   "metadata": {},
   "source": [
    "## Installation & imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0323a95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U degirum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851936b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import degirum as dg\n",
    "import degirum_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d554561",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0213888b",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 1000\n",
    "model_zoo = 'degirum/hailo'\n",
    "inference_host_address = '@local'\n",
    "single_context_model = 'yolov8n_coco--640x640_quant_hailort_multidevice_1'\n",
    "multi_context_model = 'yolov8m_coco--640x640_quant_hailort_multidevice_1'\n",
    "token = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4c1b04",
   "metadata": {},
   "source": [
    "## Eager batching on single context model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc5e0c8",
   "metadata": {},
   "source": [
    "#### batch size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1ca588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = dg.load_model(\n",
    "    model_name=single_context_model,\n",
    "    inference_host_address=inference_host_address,\n",
    "    zoo_url=\"https://hub.degirum.com/degirum/hailo\",\n",
    "    eager_batch_size=1,  # Set eager batch size to 1 for single context model\n",
    ")\n",
    "\n",
    "# Turn off C++-based post-processing (Does not affect models with a 'PythonFile' python-based postprocessor!)\n",
    "model.output_postprocess_type = \"None\"\n",
    "\n",
    "single_context_bs_1_results = degirum_tools.model_time_profile(model, iterations)\n",
    "print(f\"Running with batch size - {str(model._model_parameters.EagerBatchSize)}, Observed FPS: {single_context_bs_1_results.observed_fps:5.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a595fe",
   "metadata": {},
   "source": [
    "#### batch size 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf33c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = dg.load_model(\n",
    "    model_name=single_context_model,\n",
    "    inference_host_address=inference_host_address,\n",
    "    zoo_url=\"https://hub.degirum.com/degirum/hailo\",\n",
    "    eager_batch_size=8,  # Set eager batch size to 8 for single context model\n",
    ")\n",
    "\n",
    "# Turn off C++-based post-processing (Does not affect models with a 'PythonFile' python-based postprocessor!)\n",
    "model.output_postprocess_type = \"None\"\n",
    "\n",
    "single_context_bs_8_results = degirum_tools.model_time_profile(model, iterations)\n",
    "print(f\"Running with batch size - {str(model._model_parameters.EagerBatchSize)}, Observed FPS: {single_context_bs_8_results.observed_fps:5.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0270bd",
   "metadata": {},
   "source": [
    "### Visualise the results for single context model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ff874d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Labels and data\n",
    "labels = ['FPS - bs 1', 'FPS - bs 8']\n",
    "values = [single_context_bs_1_results.observed_fps, single_context_bs_8_results.observed_fps]\n",
    "\n",
    "# Plotting\n",
    "plt.bar(labels, values, color=['blue', 'orange'])\n",
    "\n",
    "# Customizing\n",
    "plt.ylabel('FPS')\n",
    "plt.title('Impact of batch size on single context model')\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fca6f61",
   "metadata": {},
   "source": [
    "## Eager batching on multi context model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2410506b",
   "metadata": {},
   "source": [
    "#### batch size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e8d4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model_a = dg.load_model(\n",
    "    model_name=multi_context_model,\n",
    "    inference_host_address=inference_host_address,\n",
    "    zoo_url=\"https://hub.degirum.com/degirum/hailo\",\n",
    "    eager_batch_size=1 # Set eager batch size to 1 for multi context model\n",
    ")\n",
    "\n",
    "# Turn off C++-based post-processing (Does not affect models with a 'PythonFile' python-based postprocessor!)\n",
    "model_a.output_postprocess_type = \"None\"\n",
    "\n",
    "multi_context_bs_1_results = degirum_tools.model_time_profile(model_a, iterations)\n",
    "print(f\"Running with batch size - {str(model_a._model_parameters.EagerBatchSize)}, Observed FPS: {multi_context_bs_1_results.observed_fps:5.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78019008",
   "metadata": {},
   "source": [
    "#### batch size 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59db9354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model_b = dg.load_model(\n",
    "    model_name=multi_context_model,\n",
    "    inference_host_address=inference_host_address,\n",
    "    zoo_url=\"https://hub.degirum.com/degirum/hailo\",\n",
    "    eager_batch_size=8  # Set eager batch size to 8 for multi context model \n",
    ")\n",
    "\n",
    "\n",
    "# Turn off C++-based post-processing (Does not affect models with a 'PythonFile' python-based postprocessor!)\n",
    "model_b.output_postprocess_type = \"None\"\n",
    "\n",
    "multi_context_bs_8_results = degirum_tools.model_time_profile(model_b, iterations)\n",
    "print(f\"Running with batch size - {str(model_b._model_parameters.EagerBatchSize)}, Observed FPS: {multi_context_bs_8_results.observed_fps:5.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be10733",
   "metadata": {},
   "source": [
    "### Visualise the results for multi context models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76f52da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Labels and data\n",
    "labels = ['FPS - bs 1', 'FPS - bs 8']\n",
    "values = [multi_context_bs_1_results.observed_fps, multi_context_bs_8_results.observed_fps]\n",
    "\n",
    "# Plotting\n",
    "plt.bar(labels, values, color=['blue', 'orange'])\n",
    "\n",
    "# Customizing\n",
    "plt.ylabel('FPS')\n",
    "plt.title('Impact of batch size on multi-context model')\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d88eb1d",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Eager batching can significantly improve inference performance on Hailo devices—but only for models that use multiple contexts due to their larger size. For smaller models that fully utilize the accelerator memory, batching adds no benefit and may introduce unnecessary latency. By understanding model context and evaluating FPS under different batch sizes, you can make informed decisions to optimize throughput based on your specific use case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99abf370-806d-4162-b486-403e5dd4263c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (python312env)",
   "language": "python",
   "name": "python312env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
