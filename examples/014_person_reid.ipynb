{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98d9b7ce",
   "metadata": {},
   "source": [
    "# Person Re-identification using PySDK\n",
    "This notebook demonstrates Person Re-Identification (Re-ID) using PySDK. Re-ID focuses on recognizing and matching people across different camera views based on their unique appearance, like clothing and body shape.\n",
    "\n",
    "The basic pipeline works like this:\n",
    "1. Detect people in the image using a person detection model.\n",
    "2. Crop each detected person using the bounding box coordinates.\n",
    "3. Apply the Person Re-ID model to the cropped images to extract the embeddings which can further be used to identify and match individuals across different images or camera views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802f0925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import degirum as dg, degirum_tools\n",
    "\n",
    "inference_host_address = \"@local\"\n",
    "zoo_url = \"degirum/hailo\"\n",
    "token = '' \n",
    "device_type = \"HAILORT/HAILO8L\"\n",
    "\n",
    "# Person detection model name \n",
    "person_det_model_name = \"yolov8n_relu6_person--640x640_quant_hailort_hailo8l_1\"\n",
    "\n",
    "# load AI model\n",
    "person_det_model = dg.load_model(\n",
    "    model_name=person_det_model_name,\n",
    "    inference_host_address=inference_host_address,\n",
    "    zoo_url=zoo_url,\n",
    "    token=token,\n",
    "    device_type=device_type\n",
    "    \n",
    ")\n",
    "\n",
    "# Choose the Person reid model name \n",
    "person_reid_model_name = \"osnet_x1_0_person_reid--256x128_quant_hailort_hailo8l_1\"\n",
    "# person_reid_model_name = \"repvgg_a0_person_reid--256x128_quant_hailort_hailo8l_1\" \n",
    "\n",
    "# load AI model\n",
    "person_reid_model = dg.load_model(\n",
    "    model_name=person_reid_model_name,\n",
    "    inference_host_address=inference_host_address,\n",
    "    zoo_url=zoo_url,\n",
    "    token=token,\n",
    "    device_type=device_type\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0e54d3",
   "metadata": {},
   "source": [
    "#### Let's walk through a practical example of person re-identification.<br> \n",
    "\n",
    "Letâ€™s walk through a hands-on example of how person re-identification (ReID) works.\n",
    "\n",
    "We have two set of images:\n",
    "\n",
    " * Set A: Multiple images of the same individual, captured from different camera angles.\n",
    "\n",
    " * Set B: Multiple images of a different individual, also taken from various angles.\n",
    "\n",
    "The goal is to verify whether the ReID model can:\n",
    "\n",
    "1. Correctly match all images in Set A as the same person.\n",
    "\n",
    "2. Correctly distinguish Set B as a different person from Set A.\n",
    "\n",
    "This simulates a real-world scenario where the model must recognize individuals across different camera views and lighting conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84e4c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image sources\n",
    "image_source_1 = \"../assets/person_1_multi_view.png\"\n",
    "image_source_2 = \"../assets/person_2_multi_view.png\"\n",
    "\n",
    "# Detections\n",
    "detections_1 = person_det_model(image_source_1)\n",
    "detections_2 = person_det_model(image_source_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7ecd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "detections_1, detections_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa9b414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for displaying crops\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_images(images, titles=\"Images\", figsize=(15, 5)):\n",
    "    \"\"\"\n",
    "    Display a list of images in a single row using Matplotlib.\n",
    "    \n",
    "    Parameters:\n",
    "    - images (list): List of images (NumPy arrays) to display.\n",
    "    - titles (str or list): Either a single string for overall title, or list of titles for each image.\n",
    "    - figsize (tuple): Size of the figure.\n",
    "    \"\"\"\n",
    "    num_images = len(images)\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=figsize)\n",
    "    if num_images == 1:\n",
    "        axes = [axes]  # Make iterable for single image\n",
    "\n",
    "    for i, (ax, image) in enumerate(zip(axes, images)):\n",
    "        image_rgb = image[:, :, ::-1]  # Convert BGR to RGB\n",
    "        ax.imshow(image_rgb)\n",
    "        ax.axis('off')\n",
    "        if isinstance(titles, list) and i < len(titles):\n",
    "            ax.set_title(titles[i], fontsize=12)\n",
    "\n",
    "    if isinstance(titles, str):\n",
    "        fig.suptitle(titles, fontsize=16)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d4dc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cropping\n",
    "\n",
    "# Crops from the image_source_1\n",
    "x1, y1, x2, y2 = map(int, detections_1.results[0][\"bbox\"])  # Convert bbox coordinates to integers\n",
    "crop_11 = detections_1.image[y1:y2, x1:x2]  # Crop the person from the image\n",
    "\n",
    "x1, y1, x2, y2 = map(int, detections_1.results[1][\"bbox\"])  # Convert bbox coordinates to integers\n",
    "crop_12 = detections_1.image[y1:y2, x1:x2]  # Crop the person from the image\n",
    "\n",
    "# Crops from the image_source_2\n",
    "x1, y1, x2, y2 = map(int, detections_2.results[0][\"bbox\"])  # Convert bbox coordinates to integers\n",
    "crop_21 = detections_2.image[y1:y2, x1:x2]  # Crop the person from the image\n",
    "\n",
    "x1, y1, x2, y2 = map(int, detections_2.results[1][\"bbox\"])  # Convert bbox coordinates to integers\n",
    "crop_22 = detections_2.image[y1:y2, x1:x2]  # Crop the person from the image\n",
    "\n",
    "# Display person crops\n",
    "display_images([crop_11, crop_12, crop_21, crop_22], titles=[\"Crop11\",\"Crop12\",\"Crop21\", \"Crop22\"], figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba56d3e8",
   "metadata": {},
   "source": [
    "### Extracting embedding using a Person recognition model for each person crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d782035a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the embeddings for the image_source_1 that has two crops\n",
    "embedding_11 = person_reid_model(crop_11).results[0][\"data\"][0][0] # shape (1,512)\n",
    "embedding_12 = person_reid_model(crop_12).results[0][\"data\"][0][0] # shape (1,512)\n",
    "\n",
    "#Extract the embeddings for the image_source_2 that detected two crops\n",
    "embedding_21 = person_reid_model(crop_21).results[0][\"data\"][0][0] # shape (1,512)\n",
    "embedding_22 = person_reid_model(crop_22).results[0][\"data\"][0][0] # shape (1,512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8cb4b9",
   "metadata": {},
   "source": [
    "### Calculating cosine similarity between the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dbb1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compute cosine similarity between crops of the same person on different camera views\n",
    "similarity_1 = cosine_similarity(embedding_11, embedding_12)\n",
    "print(\"Cosine similarity between crop11 and crop12 (two images of the same person with different camera views):\",similarity_1[0][0])\n",
    "\n",
    "similarity_1 = cosine_similarity(embedding_21, embedding_22)\n",
    "print(\"Cosine similarity between crop21 and crop22 (two images of the same person with different camera views):\", similarity_1[0][0], \"\\n\")\n",
    "\n",
    "\n",
    "# Compute cosine similarity between crops of two different people\n",
    "similarity_2 = cosine_similarity(embedding_11, embedding_21)\n",
    "print(\"Cosine similarity between crop11 and crop21 (two different people):\", similarity_2[0][0])\n",
    "\n",
    "similarity_2 = cosine_similarity(embedding_12, embedding_22)\n",
    "print(\"Cosine similarity between crop12 and crop22 (two different people):\", similarity_2[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237de308",
   "metadata": {},
   "source": [
    "### The results show that the cosine similarity between images of the same person (captured from different camera angles) is significantly higher than the similarity between images of two different individuals."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
