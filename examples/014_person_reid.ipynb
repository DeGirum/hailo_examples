{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98d9b7ce",
   "metadata": {},
   "source": [
    "# Person Re-identification using PySDK\n",
    "This notebook demonstrates Person Re-Identification (Re-ID) using PySDK. Re-ID focuses on recognizing and matching people across different camera views based on their unique appearance, like clothing and body shape.\n",
    "\n",
    "The basic pipeline works like this:\n",
    "1. Detect people in the image using a person detection model.\n",
    "2. Crop each detected person using the bounding box coordinates.\n",
    "3. Apply the Person Re-ID model to the cropped images to extract the embeddings which can further be used to identify and match individuals across different images or camera views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802f0925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import degirum as dg, degirum_tools\n",
    "\n",
    "inference_host_address = \"@local\"\n",
    "zoo_url = \"degirum/hailo\"\n",
    "token = '' \n",
    "device_type = \"HAILORT/HAILO8L\"\n",
    "\n",
    "# Person detection model name \n",
    "person_det_model_name = \"yolov8n_relu6_person--640x640_quant_hailort_hailo8l_1\"\n",
    "\n",
    "# load AI model\n",
    "person_det_model = dg.load_model(\n",
    "    model_name=person_det_model_name,\n",
    "    inference_host_address=inference_host_address,\n",
    "    zoo_url=zoo_url,\n",
    "    token=token,\n",
    "    device_type=device_type\n",
    "    \n",
    ")\n",
    "\n",
    "# Choose the Person reid model name \n",
    "person_reid_model_name = \"osnet_x1_0_person_reid--256x128_quant_hailort_hailo8l_1\"\n",
    "# person_reid_model_name = \"repvgg_a0_person_reid--256x128_quant_hailort_hailo8l_1\" \n",
    "\n",
    "# load AI model\n",
    "person_reid_model = dg.load_model(\n",
    "    model_name=person_reid_model_name,\n",
    "    inference_host_address=inference_host_address,\n",
    "    zoo_url=zoo_url,\n",
    "    token=token,\n",
    "    device_type=device_type\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0e54d3",
   "metadata": {},
   "source": [
    "#### Let's walk through a practical example of person re-identification.<br> \n",
    "\n",
    "We have three input images:\n",
    "\n",
    "- **Two images of the same individual** (Monica)\n",
    "- **One image of a different individual** (Phoebe)\n",
    "\n",
    "The goal is to verify whether the model can correctly identify that the first two images belong to the same person and distinguish the third image as a different person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84e4c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image sources\n",
    "image_source_monica_1 = \"../assets/Friends_dataset/Monica_2.jpg\"\n",
    "image_source_monica_2 = \"../assets/Friends_dataset/Monica_4.png\"\n",
    "image_source_phoebe = \"../assets/Friends_dataset/Phoebe_4.png\"\n",
    "\n",
    "# Detections\n",
    "detections_monica_1 = person_det_model(image_source_monica_1)\n",
    "detections_monica_2 = person_det_model(image_source_monica_2)\n",
    "detections_phoebe = person_det_model(image_source_phoebe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bb1e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for displaying crops\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_images(images, titles=\"Images\", figsize=(15, 5)):\n",
    "    \"\"\"\n",
    "    Display a list of images in a single row using Matplotlib.\n",
    "    \n",
    "    Parameters:\n",
    "    - images (list): List of images (NumPy arrays) to display.\n",
    "    - titles (str or list): Either a single string for overall title, or list of titles for each image.\n",
    "    - figsize (tuple): Size of the figure.\n",
    "    \"\"\"\n",
    "    num_images = len(images)\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=figsize)\n",
    "    if num_images == 1:\n",
    "        axes = [axes]  # Make iterable for single image\n",
    "\n",
    "    for i, (ax, image) in enumerate(zip(axes, images)):\n",
    "        image_rgb = image[:, :, ::-1]  # Convert BGR to RGB\n",
    "        ax.imshow(image_rgb)\n",
    "        ax.axis('off')\n",
    "        if isinstance(titles, list) and i < len(titles):\n",
    "            ax.set_title(titles[i], fontsize=12)\n",
    "\n",
    "    if isinstance(titles, str):\n",
    "        fig.suptitle(titles, fontsize=16)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d4dc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cropping\n",
    "x1, y1, x2, y2 = map(int, detections_monica_1.results[0][\"bbox\"])  # Convert bbox coordinates to integers\n",
    "monica_crop_1 = detections_monica_1.image[y1:y2, x1:x2]  # Crop the person from the image\n",
    "\n",
    "x1, y1, x2, y2 = map(int, detections_monica_2.results[0][\"bbox\"])  # Convert bbox coordinates to integers\n",
    "monica_crop_2 = detections_monica_2.image[y1:y2, x1:x2]  # Crop the person from the image\n",
    "\n",
    "x1, y1, x2, y2 = map(int, detections_phoebe.results[0][\"bbox\"])  # Convert bbox coordinates to integers\n",
    "phoebe_crop = detections_phoebe.image[y1:y2, x1:x2]  # Crop the person from the image\n",
    "\n",
    "# Display person crops\n",
    "display_images([monica_crop_1, monica_crop_2, phoebe_crop], titles=[\"Monica1\",\"Monica2\",\"Phoebe\"], figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce7c92f",
   "metadata": {},
   "source": [
    "### Extracting embedding using a Person recognition model for each person crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d782035a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the embeddings\n",
    "monica_embedding_1 = person_reid_model(monica_crop_1).results[0][\"data\"][0][0] # shape (1,512)\n",
    "monica_embedding_2 = person_reid_model(monica_crop_2).results[0][\"data\"][0][0] # shape (1,512)\n",
    "phoebe_embedding = person_reid_model(phoebe_crop).results[0][\"data\"][0][0] # shape (1,512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8cb4b9",
   "metadata": {},
   "source": [
    "### Calculating cosine similarity between the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dbb1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compute cosine similarity between two images of Monica\n",
    "similarity_1 = cosine_similarity(monica_embedding_1, monica_embedding_2)\n",
    "print(\"Cosine similarity between two images of Monica: \", similarity_1[0][0])\n",
    "\n",
    "# Compute cosine similarity between Monica and Phoebe\n",
    "similarity_2 = cosine_similarity(monica_embedding_1, phoebe_embedding)\n",
    "print(\"Cosine similarity between Monica and Phoebe: \", similarity_2[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237de308",
   "metadata": {},
   "source": [
    "### The results show that the two images of Monica are the same person as the cosine similarity is larger than the one compared between two images of two different people."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
