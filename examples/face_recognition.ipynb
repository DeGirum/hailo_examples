{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dff11d6",
   "metadata": {},
   "source": [
    "## A Comprehensive Guide to Building a Face Recognition System\n",
    "A face recognition system operates through a sequence of well-defined stages, each essential for ensuring accurate and reliable results:\n",
    "\n",
    "1. **Face Detection and Keypoint Extraction**: The process begins with detecting faces in an image or video using specialized models. These models not only identify the bounding boxes of faces but also detect key facial landmarks, such as the eyes, nose, and mouth. These landmarks are crucial for ensuring accurate alignment in the next step.\n",
    "   \n",
    "   Several face detection models are widely used in modern pipelines:\n",
    "   - **SCRFD**: A lightweight and efficient model optimized for speed and accuracy, ideal for resource-constrained environments.\n",
    "   - **RetinaFace**: Known for its high accuracy, RetinaFace detects five key landmarks along with bounding boxes, making it a popular choice for robust alignment.\n",
    "   - **YOLOv8-based Models**: Adapted for face detection tasks, these models provide state-of-the-art performance with keypoint detection capabilities.\n",
    "\n",
    "   These models handle variations in lighting, pose, and scale, ensuring reliable detection even in challenging conditions.\n",
    "\n",
    "2. **Face Alignment**: Detected faces are standardized in orientation and size using the extracted keypoints. This process involves rotating, scaling, and cropping faces to ensure that features like the eyes and mouth are consistently positioned. Alignment reduces variations caused by pose or tilt, making the inputs more consistent for subsequent steps. By aligning faces, the system ensures that the embedding model focuses on relevant features, improving the quality and reliability of the embeddings.\n",
    "\n",
    "3. **Embedding Extraction**: Once aligned, the faces are passed through an embedding model, such as **ArcFace-MobileFaceNet**, which generates a high-dimensional feature vector (e.g., 512 dimensions) representing each face. These embeddings encode the unique characteristics of the face, enabling efficient comparison.\n",
    "\n",
    "4. **Database Matching**: The generated embeddings are compared against a database of known embeddings using similarity metrics such as cosine similarity. If the similarity exceeds a predefined threshold, the system identifies or verifies the individual."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9280fb9f",
   "metadata": {},
   "source": [
    "### Stage 1: Face Detection with Keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ab7da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import degirum as dg\n",
    "import degirum_tools\n",
    "\n",
    "# Choose the model name \n",
    "face_det_model_name = \"scrfd_10g--640x640_quant_hailort_hailo8l_1\"\n",
    "# face_det_model_name = \"scrfd_2.5g--640x640_quant_hailort_hailo8l_1\"\n",
    "# face_det_model_name = \"scrfd_500m--640x640_quant_hailort_hailo8l_1\"\n",
    "# face_det_model_name = \"yolov8n_relu6_widerface_kpts--640x640_quant_hailort_hailo8l_1\"\n",
    "face_det_model_name = \"retinaface_mobilenet--736x1280_quant_hailort_hailo8l_1\"\n",
    "\n",
    "# Specify the inference host address\n",
    "inference_host_address = \"@cloud\"  # Use \"@cloud\" for cloud inference\n",
    "# inference_host_address = \"@local\"  # Use \"@local\" for local inference\n",
    "\n",
    "# Specify the zoo_url\n",
    "zoo_url = \"degirum/models_hailort\"\n",
    "# zoo_url = \"<path to local folder>\"  # For local model files\n",
    "\n",
    "# Specify the image source\n",
    "image_source = \"../assets/Friends1.jpg\"\n",
    "\n",
    "# Set the token for accessing the inference service\n",
    "token = degirum_tools.get_token()\n",
    "# token = ''  # Leave empty for local inference\n",
    "\n",
    "# Load the face detection model\n",
    "face_det_model = dg.load_model(\n",
    "    model_name=face_det_model_name,\n",
    "    inference_host_address=inference_host_address,\n",
    "    zoo_url=zoo_url,\n",
    "    token=token, \n",
    "    overlay_color=(0, 255, 0)  # Green color for bounding boxes\n",
    ")\n",
    "\n",
    "# Run the inference\n",
    "detected_faces = face_det_model(image_source)\n",
    "print(detected_faces)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b198c54d",
   "metadata": {},
   "source": [
    "#### Utility Function to Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a2d9f145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def display_images(images, title=\"Images\", figsize=(15, 5)):\n",
    "    \"\"\"\n",
    "    Display a list of images in a single row using Matplotlib.\n",
    "\n",
    "    Parameters:\n",
    "    - images (list): List of images (NumPy arrays) to display.\n",
    "    - title (str): Title for the plot.\n",
    "    - figsize (tuple): Size of the figure.\n",
    "    \"\"\"\n",
    "    num_images = len(images)\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=figsize)\n",
    "    if num_images == 1:\n",
    "        axes = [axes]  # Make it iterable for a single image\n",
    "    for ax, image in zip(axes, images):\n",
    "        image_rgb = image[:, :, ::-1]  # Convert BGR to RGB\n",
    "        ax.imshow(image_rgb)\n",
    "        ax.axis('off')\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c74865",
   "metadata": {},
   "source": [
    "#### Visualizing Face Detection results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ce8c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images([detected_faces.image_overlay], title=\"Face Detection Result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98e382a",
   "metadata": {},
   "source": [
    "#### Visualizing Cropped Faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5c24c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store cropped and aligned faces\n",
    "cropped_faces = []\n",
    "\n",
    "# Process each detection result\n",
    "for face in detected_faces.results:\n",
    "    # Extract bounding box (assumed in [x1, y1, x2, y2] format)\n",
    "    x1, y1, x2, y2 = map(int, face[\"bbox\"])  # Convert bbox coordinates to integers\n",
    "    cropped_face = detected_faces.image[y1:y2, x1:x2]  # Crop the face from the image\n",
    "    cropped_faces.append(cropped_face)\n",
    "\n",
    "# Display cropped faces\n",
    "display_images(cropped_faces, title=\"Cropped Faces\", figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556fade7",
   "metadata": {},
   "source": [
    "### Stage 2: Face Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "00ea2add",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def align_and_crop(img, landmarks, image_size=112):\n",
    "    \"\"\"\n",
    "    Align and crop the face from the image based on the given landmarks.\n",
    "\n",
    "    Args:\n",
    "        img (np.ndarray): The full image (not the cropped bounding box). This image will be transformed.\n",
    "        landmarks (List[np.ndarray]): List of 5 keypoints (landmarks) as (x, y) coordinates. These keypoints typically include the eyes, nose, and mouth.\n",
    "        image_size (int, optional): The size to which the image should be resized. Defaults to 112. It is typically either 112 or 128 for face recognition models.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, np.ndarray]: The aligned face image and the transformation matrix.\n",
    "    \"\"\"\n",
    "    # Define the reference keypoints used in ArcFace model, based on a typical facial landmark set.\n",
    "    _arcface_ref_kps = np.array(\n",
    "        [\n",
    "            [38.2946, 51.6963],  # Left eye\n",
    "            [73.5318, 51.5014],  # Right eye\n",
    "            [56.0252, 71.7366],  # Nose\n",
    "            [41.5493, 92.3655],  # Left mouth corner\n",
    "            [70.7299, 92.2041],  # Right mouth corner\n",
    "        ],\n",
    "        dtype=np.float32,\n",
    "    )\n",
    "\n",
    "    # Ensure the input landmarks have exactly 5 points (as expected for face alignment)\n",
    "    assert len(landmarks) == 5\n",
    "\n",
    "    # Validate that image_size is divisible by either 112 or 128 (common image sizes for face recognition models)\n",
    "    assert image_size % 112 == 0 or image_size % 128 == 0\n",
    "\n",
    "    # Adjust the scaling factor (ratio) based on the desired image size (112 or 128)\n",
    "    if image_size % 112 == 0:\n",
    "        ratio = float(image_size) / 112.0\n",
    "        diff_x = 0  # No horizontal shift for 112 scaling\n",
    "    else:\n",
    "        ratio = float(image_size) / 128.0\n",
    "        diff_x = 8.0 * ratio  # Horizontal shift for 128 scaling\n",
    "\n",
    "    # Apply the scaling and shifting to the reference keypoints\n",
    "    dst = _arcface_ref_kps * ratio\n",
    "    dst[:, 0] += diff_x  # Apply the horizontal shift\n",
    "\n",
    "    # Estimate the similarity transformation matrix to align the landmarks with the reference keypoints\n",
    "    M, inliers = cv2.estimateAffinePartial2D(np.array(landmarks), dst, ransacReprojThreshold=1000)\n",
    "    assert np.all(inliers == True)\n",
    "\n",
    "    # Apply the affine transformation to the input image to align the face\n",
    "    aligned_img = cv2.warpAffine(img, M, (image_size, image_size), borderValue=0.0)\n",
    "\n",
    "    return aligned_img, M\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfa51d6",
   "metadata": {},
   "source": [
    "#### Visualizing Aligned Faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecebcfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store aligned faces\n",
    "aligned_faces = []\n",
    "\n",
    "# Process each detection result\n",
    "for face in detected_faces.results:\n",
    "    # Extract landmarks and align the face\n",
    "    landmarks = [landmark[\"landmark\"] for landmark in face[\"landmarks\"]]\n",
    "    aligned_face, _ = align_and_crop(detected_faces.image, landmarks)  # Align and crop face\n",
    "    aligned_faces.append(aligned_face)\n",
    "\n",
    "# Display aligned faces\n",
    "display_images(aligned_faces, title=\"Aligned Faces\", figsize=(10, 5))   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e14af0",
   "metadata": {},
   "source": [
    "### Stage 3: Extracting Embedding using a Face Recognition Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f4707a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Face recognition model name\n",
    "face_rec_model_name = \"arcface_mobilefacenet--112x112_quant_hailort_hailo8l_1\"\n",
    "\n",
    "# Load the face recognition model\n",
    "face_rec_model = dg.load_model(\n",
    "    model_name=face_rec_model_name,\n",
    "    inference_host_address=inference_host_address,\n",
    "    zoo_url=zoo_url,\n",
    "    token=token\n",
    ")\n",
    "\n",
    "# Process each detected face\n",
    "for face in detected_faces.results:\n",
    "    # Extract landmarks and align the face\n",
    "    landmarks = [landmark[\"landmark\"] for landmark in face[\"landmarks\"]]\n",
    "    aligned_face, _ = align_and_crop(detected_faces.image, landmarks)  # Align and crop face\n",
    "    face_embedding = face_rec_model(aligned_face).results[0][\"data\"][0]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76342ce6",
   "metadata": {},
   "source": [
    "### Stage 4: Database Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fa03df",
   "metadata": {},
   "source": [
    "#### Database Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "43e50a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lancedb.pydantic import LanceModel, Vector\n",
    "import uuid\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "\n",
    "class FaceRecognitionSchema(LanceModel):\n",
    "    id: str  # Unique identifier for each entry\n",
    "    vector: Vector(512)  # Face embeddings, fixed size of 512\n",
    "    entity_name: str  # Name of the entity\n",
    "\n",
    "    @classmethod\n",
    "    def prepare_face_records(cls, face_embeddings: List[Dict], entity_name: str) -> List['FaceRecognitionSchema']:\n",
    "        \"\"\"\n",
    "        Converts a list of face detection results to a list of FaceRecognitionSchema instances.\n",
    "\n",
    "        Args:\n",
    "            face_embeddings (List[Dict]): List of face embeddings.\n",
    "            entity_name (str): Name of the entity.\n",
    "\n",
    "        Returns:\n",
    "            List[FaceRecognitionSchema]: List of formatted instances.\n",
    "        \"\"\"\n",
    "        if not face_embeddings:\n",
    "            return []\n",
    "\n",
    "        formatted_records = []\n",
    "        for embedding in face_embeddings:\n",
    "            formatted_records.append(\n",
    "                cls(\n",
    "                    id=str(uuid.uuid4()),  # Generate a unique ID\n",
    "                    vector=np.array(embedding, dtype=np.float32),  # Convert embedding to float32 numpy array\n",
    "                    entity_name=entity_name\n",
    "                )\n",
    "            )\n",
    "        return formatted_records\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b733bcc",
   "metadata": {},
   "source": [
    "#### Function to Populate the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d1af679e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import Any\n",
    "\n",
    "# Configure logging for better output control\n",
    "logging.basicConfig(level=logging.WARNING, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "def populate_database_from_images(\n",
    "    input_path: str,\n",
    "    face_det_model: Any,\n",
    "    face_rec_model: Any,\n",
    "    tbl: Any  # LanceDB table object\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Iterates over images in a directory, detects faces, generates embeddings,\n",
    "    and populates the database with face records.\n",
    "\n",
    "    Args:\n",
    "        input_path (str): Path to the directory containing image files.\n",
    "        face_det_model (Any): Face detection and keypoints model.\n",
    "        face_rec_model (Any): Face re-identification model.\n",
    "        tbl (Any): LanceDB table object.\n",
    "    \"\"\"\n",
    "    path = Path(input_path)\n",
    "    num_entities = 0  # Counter for the number of entities added to the database\n",
    "\n",
    "    # Find all image files in the directory and subdirectories\n",
    "    image_files = [str(file) for file in path.rglob(\"*\") if file.suffix.lower() in (\".png\", \".jpg\", \".jpeg\")]\n",
    "    identities = [file.stem.split(\"_\")[0] for file in path.rglob(\"*\") if file.suffix.lower() in (\".png\", \".jpg\", \".jpeg\")]\n",
    "    print(image_files, identities)\n",
    "    if not image_files:\n",
    "        logging.warning(f\"No image files found in {input_path}.\")\n",
    "        return\n",
    "\n",
    "    for identity, detected_faces in zip(identities, face_det_model.predict_batch(image_files)):\n",
    "        try:\n",
    "            print(identity, detected_faces.info)\n",
    "            # Count number of detected faces\n",
    "            num_faces = len(detected_faces.results)\n",
    "\n",
    "            # Skip images with more than one face\n",
    "            if num_faces > 1:\n",
    "                logging.warning(f\"Skipped {detected_faces.info} as it contains more than one face ({num_faces} faces detected).\")\n",
    "                continue\n",
    "            elif num_faces == 0:\n",
    "                logging.warning(f\"Skipped {detected_faces.info} as no faces were detected.\")\n",
    "                continue\n",
    "\n",
    "            # Process the single detected face\n",
    "            result = detected_faces.results[0]\n",
    "\n",
    "            # Generate face embedding\n",
    "            aligned_img, _ = align_and_crop(detected_faces.image, [landmark[\"landmark\"] for landmark in result[\"landmarks\"]])\n",
    "            face_embedding = face_rec_model(aligned_img).results[0][\"data\"][0]\n",
    "            \n",
    "            # Prepare records for the database\n",
    "            records = FaceRecognitionSchema.prepare_face_records([face_embedding], identity)\n",
    "\n",
    "            # Add records to the database if valid\n",
    "            if records:\n",
    "                tbl.add(data=records)\n",
    "                num_entities += len(records)\n",
    "            else:\n",
    "                logging.warning(f\"No valid records generated for {detected_faces.info}.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing {file}: {e}\", exc_info=True)\n",
    "\n",
    "    # Log summary\n",
    "    logging.info(f\"Successfully added {num_entities} entities to the database table.\")\n",
    "    total_entities = tbl.count_rows()\n",
    "    logging.info(f\"The table now contains {total_entities} entities.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fc39ff",
   "metadata": {},
   "source": [
    "#### Building the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd82b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lancedb\n",
    "\n",
    "# Database and table setup\n",
    "uri = \"../.temp/face_database\"\n",
    "table_name = \"face\"\n",
    "\n",
    "# Path to the directory containing the sample dataset for indexing.\n",
    "input_path = \"../assets/Friends_dataset\"\n",
    "\n",
    "# Connect to the database\n",
    "db = lancedb.connect(uri=uri)\n",
    "\n",
    "# Initialize the table\n",
    "if table_name not in db.table_names():\n",
    "    tbl = db.create_table(table_name, schema=FaceRecognitionSchema)\n",
    "else:\n",
    "    tbl = db.open_table(table_name)\n",
    "    schema_fields = [field.name for field in tbl.schema]\n",
    "    if schema_fields != list(FaceRecognitionSchema.model_fields.keys()):\n",
    "        raise RuntimeError(f\"Table {table_name} has a different schema.\")\n",
    "\n",
    "# Process images and populate the database\n",
    "populate_database_from_images(\n",
    "    input_path=input_path,\n",
    "    face_det_model=face_det_model,\n",
    "    face_rec_model=face_rec_model,\n",
    "    tbl=tbl\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9093a1ac",
   "metadata": {},
   "source": [
    "#### Function to Query the Database for the Closest Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f0af7536",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Any\n",
    "import numpy as np\n",
    "\n",
    "def identify_faces(\n",
    "    embeddings: List[np.ndarray],  # List of NumPy arrays representing face embeddings\n",
    "    tbl: Any,                      # The database or table object supporting the search method\n",
    "    field_name: str,               # Name of the vector column in the database\n",
    "    metric_type: str,              # Metric type for distance calculation (e.g., \"cosine\", \"euclidean\")\n",
    "    top_k: int,                    # Number of top results to fetch from the database\n",
    "    threshold: float = 0.3         # Distance threshold for assigning labels\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Identifies faces by searching for the nearest embeddings in the database and assigning labels.\n",
    "\n",
    "    Args:\n",
    "        embeddings (List[np.ndarray]): List of NumPy arrays representing face embeddings.\n",
    "        tbl (Any): Database or table object supporting search functionality.\n",
    "        field_name (str): Name of the vector column to search against.\n",
    "        metric_type (str): Distance metric to use (e.g., \"cosine\", \"euclidean\").\n",
    "        top_k (int): Number of top results to retrieve.\n",
    "        threshold (float, optional): Minimum similarity score for assigning a known label. Defaults to 0.3.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of labels for the provided embeddings. Returns \"Unknown\" for embeddings below the threshold.\n",
    "    \"\"\"\n",
    "    identities = []  # List to store the assigned labels\n",
    "    similarity_scores = []  # List to store the similarity scores\n",
    "\n",
    "    for embedding in embeddings:\n",
    "        # Perform database search\n",
    "        search_result = (\n",
    "            tbl.search(\n",
    "                embedding,\n",
    "                vector_column_name=field_name\n",
    "            )\n",
    "            .metric(metric_type)\n",
    "            .limit(top_k)\n",
    "            .to_list()\n",
    "        )\n",
    "\n",
    "        # Check if search_result has any entries\n",
    "        if not search_result:\n",
    "            identities.append(\"Unknown\")\n",
    "            continue\n",
    "\n",
    "        # Calculate the similarity score\n",
    "        similarity_score = round(1 - search_result[0][\"_distance\"], 2)\n",
    "\n",
    "        # Assign a label based on the similarity threshold\n",
    "        identity = search_result[0][\"entity_name\"] if similarity_score >= threshold else \"Unknown\"\n",
    "\n",
    "        # Append the label to the results list\n",
    "        identities.append(identity)\n",
    "        similarity_scores.append(similarity_score)\n",
    "    return identities, similarity_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f483c02d",
   "metadata": {},
   "source": [
    "### Putting It All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cc7c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lancedb\n",
    "# database related parameters\n",
    "top_k = 1\n",
    "field_name = \"vector\"\n",
    "metric_type = \"cosine\"\n",
    "\n",
    "# Database and table parameters\n",
    "uri = \"../.temp/face_database\"\n",
    "table_name = \"face\"\n",
    "\n",
    "# Connect to the database\n",
    "db = lancedb.connect(uri=uri)\n",
    "tbl = db.open_table(table_name)\n",
    "\n",
    "# check the schema of the table to ensure it matches the expected schema\n",
    "schema_fields = [field.name for field in tbl.schema]\n",
    "if schema_fields != list(FaceRecognitionSchema.model_fields.keys()):\n",
    "    raise RuntimeError(f\"Table {table_name} has a different schema.\")\n",
    "\n",
    "# image source\n",
    "image_source = \"../assets/Friends1.jpg\"\n",
    "\n",
    "# run the face detection model\n",
    "face_det_model.overlay_show_probabilities=False\n",
    "detected_faces = face_det_model(image_source) \n",
    "\n",
    "# Process the detected faces: align, crop, extract embeddings and find the identity\n",
    "if detected_faces.results:\n",
    "    for face in detected_faces.results:\n",
    "        landmarks = [landmark[\"landmark\"] for landmark in face[\"landmarks\"]]\n",
    "        aligned_face, _ = align_and_crop(detected_faces.image, landmarks)\n",
    "        face_embedding = face_rec_model(aligned_face).results[0][\"data\"][0]\n",
    "        identities, similarity_scores = identify_faces([face_embedding], tbl, field_name, metric_type, top_k)\n",
    "        # replace the label and score in the result with the identity and similarity score\n",
    "        face[\"label\"] = identities[0]  # Assign the first label\n",
    "        face[\"score\"] = similarity_scores[0]  # Assign the first score\n",
    "\n",
    "display_images([detected_faces.image_overlay], title=\"Face Recognition Result\", figsize=(10, 5)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e965d95",
   "metadata": {},
   "source": [
    "#### Optimizing Using `predict_batch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122c767e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_source = \"../assets/Friends1.jpg\"\n",
    "detected_faces = face_det_model(image_source) \n",
    "aligned_faces = []\n",
    "if detected_faces.results:\n",
    "    for face in detected_faces.results:\n",
    "        landmarks = [landmark[\"landmark\"] for landmark in face[\"landmarks\"]]\n",
    "        aligned_face, _ = align_and_crop(detected_faces.image, landmarks)\n",
    "        aligned_faces.append(aligned_face)    \n",
    "    \n",
    "    # Run batch predict on aligned faces, find identity, assign labels and scores to each detection\n",
    "    for face, face_embedding in zip(detected_faces.results, face_rec_model.predict_batch(aligned_faces)):\n",
    "        embedding = face_embedding.results[0][\"data\"][0]  # Extract embedding\n",
    "        identities, similarity_scores = identify_faces([embedding], tbl, field_name, metric_type, top_k)\n",
    "        face[\"label\"] = identities[0]  # Assign the first label\n",
    "        face[\"score\"] = similarity_scores[0]  # Assign the first score\n",
    "\n",
    "display_images([detected_faces.image_overlay], title=\"Face Recognition Result\", figsize=(10, 5))           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf01fa9e",
   "metadata": {},
   "source": [
    "#### Running Inference on a Video Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9ebfa244",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_source = 1\n",
    "\n",
    "with degirum_tools.Display(\"AI Camera\") as output_display:\n",
    "    for detected_faces in degirum_tools.predict_stream(face_det_model, video_source):\n",
    "        if detected_faces.results:\n",
    "            aligned_faces = []\n",
    "            for face in detected_faces.results:\n",
    "                landmarks = [landmark[\"landmark\"] for landmark in face[\"landmarks\"]]\n",
    "                aligned_face, _ = align_and_crop(detected_faces.image, landmarks)\n",
    "                aligned_faces.append(aligned_face)    \n",
    "            \n",
    "            # Run batch predict on aligned faces, find identity, assign labels and scores to each detection\n",
    "            for face, face_embedding in zip(detected_faces.results, face_rec_model.predict_batch(aligned_faces)):\n",
    "                embedding = face_embedding.results[0][\"data\"][0]  # Extract embedding\n",
    "                identities, similarity_scores = identify_faces([embedding], tbl, field_name, metric_type, top_k)\n",
    "                face[\"label\"] = identities[0]  # Assign the first label\n",
    "                face[\"score\"] = similarity_scores[0]  # Assign the first score\n",
    "        output_display.show(detected_faces.image_overlay)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "supervision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
