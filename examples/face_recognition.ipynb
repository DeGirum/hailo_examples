{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a93c3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import degirum as dg, degirum_tools\n",
    "import cv2, numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6091ed2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose inference host address\n",
    "inference_host_address = \"@cloud\" \n",
    "# inference_host_address = \"@local\"\n",
    "\n",
    "# choose zoo_url\n",
    "zoo_url = \"degirum/models_hailort\"\n",
    "# zoo_url = \"<path to local folder>\"\n",
    "\n",
    "# set token\n",
    "token = degirum_tools.get_token()\n",
    "# token = '' # leave empty for local inference\n",
    "\n",
    "# choose image source\n",
    "image_source = \"../assets/Friends.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638f4cf1",
   "metadata": {},
   "source": [
    "**Face Detection and Keypoint Estimation with DeGirum PySDK** \n",
    "\n",
    "It loads a YOLOv8 face detection model with keypoint estimation and applies it to an input image. The detected faces and their corresponding keypoints are displayed visually, providing insights into facial regions. This setup is ideal for tasks such as face analysis, landmark detection, or pose estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27393e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Running inference using 'yolov8n_relu6_widerface_kpts--640x640_quant_hailort_hailo8l_1' on image source '../assets/Friends.jpg'\n",
      "Press 'x' or 'q' to stop.\n"
     ]
    }
   ],
   "source": [
    "face_det_kypts_model_name = \"yolov8n_relu6_widerface_kpts--640x640_quant_hailort_hailo8l_1\"\n",
    "\n",
    "# load AI model\n",
    "face_det_kypts_model = dg.load_model(\n",
    "    model_name=face_det_kypts_model_name,\n",
    "    inference_host_address=inference_host_address,\n",
    "    zoo_url=zoo_url,\n",
    "    token=token\n",
    ")\n",
    "\n",
    "# perform AI model inference on given image source\n",
    "print(f\" Running inference using '{face_det_kypts_model_name}' on image source '{image_source}'\")\n",
    "face_det_kypts_inference_result = face_det_kypts_model(image_source)\n",
    "\n",
    "# print('Inference Results \\n', face_det_kypts_inference_result)  # Detection Results with keypoints\n",
    "\n",
    "# show results of inference\n",
    "with degirum_tools.Display(\"AI Camera\") as output_display:\n",
    "    output_display.show_image(face_det_kypts_inference_result)\n",
    "    \n",
    "print(\"Press 'x' or 'q' to stop.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9328d58a",
   "metadata": {},
   "source": [
    "**Face Re-identification with DeGirum PySDK**\n",
    "\n",
    "This script loads a YOLOv8-based face re-identification (ReID) model and applies it to the face crops obtained from YOLOv8 face detection model with keypoint estimation. The model extracts extracts unique features/embeddings from each cropped face.\n",
    "\n",
    "The output of the ReID model is a 512-dimensional embedding, which encapsulates the unique features of each cropped face, enabling precise identification and comparison across different images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "943eb22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of a face embedding: 512\n"
     ]
    }
   ],
   "source": [
    "face_reid_model_name = \"arcface_mobilefacenet--112x112_quant_hailort_hailo8_2\"\n",
    "zoo_url = \"degirum/sandbox_shashi\"\n",
    "\n",
    "# load AI model\n",
    "face_reid_model = dg.load_model(\n",
    "    model_name=face_reid_model_name,\n",
    "    inference_host_address=inference_host_address,\n",
    "    zoo_url=zoo_url,\n",
    "    token=token\n",
    ")\n",
    "image = face_det_kypts_inference_result.image\n",
    "\n",
    "cropped_faces=[]\n",
    "for face in face_det_kypts_inference_result.results:\n",
    "    x1, y1, x2, y2 = map(int, face[\"bbox\"])\n",
    "    # Crop the face from the image using corner coordinates\n",
    "    cropped_face = image[y1:y2, x1:x2]\n",
    "    # Append the cropped face to the list\n",
    "    cropped_faces.append(cropped_face)\n",
    "\n",
    "    \n",
    "face_reid_inference_result = face_reid_model(cropped_faces[0]) # Embedding for a single cropped face\n",
    "print ('Shape of a face embedding:', len(face_reid_inference_result.results[0][\"data\"][0])) # Print the length of the embedding for a single face."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f520c38",
   "metadata": {},
   "source": [
    "#### Display the Cropped Faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c81ea5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'cropped_faces' contains a list of face images\n",
    "num_faces = len(cropped_faces)\n",
    "cols = 3  # Number of columns in the grid\n",
    "rows = (num_faces // cols) + (num_faces % cols > 0)  # Calculate rows needed\n",
    "\n",
    "# Calculate maximum width and height in one pass using zip to avoid multiple iterations\n",
    "max_width, max_height = max((img.shape[1], img.shape[0]) for img in cropped_faces)\n",
    "\n",
    "# Create a blank canvas to hold all the images in a grid layout\n",
    "canvas_width = cols * max_width\n",
    "canvas_height = rows * max_height\n",
    "canvas = np.zeros((canvas_height, canvas_width, 3), dtype=np.uint8)  # Blank canvas\n",
    "\n",
    "# Resize all images to match max width and height (if needed)\n",
    "resized_faces = [cv2.resize(face, (max_width, max_height)) for face in cropped_faces]\n",
    "\n",
    "# Place each image in the correct location on the canvas\n",
    "for idx, face in enumerate(resized_faces):\n",
    "    row = idx // cols  # Determine row number\n",
    "    col = idx % cols  # Determine column number\n",
    "\n",
    "    x_offset = col * max_width  # X position on canvas\n",
    "    y_offset = row * max_height  # Y position on canvas\n",
    "\n",
    "    # Place the face image on the canvas at the calculated position\n",
    "    canvas[y_offset:y_offset + face.shape[0], x_offset:x_offset + face.shape[1]] = face\n",
    "\n",
    "# Display the final canvas with all images in a grid\n",
    "cv2.imshow('Cropped Faces Grid', canvas)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f3259b",
   "metadata": {},
   "source": [
    "**Align and Crop**\n",
    "\n",
    "The `align_and_crop` function is designed to align and crop a face from an image based on a given set of landmarks. This is particularly useful in facial recognition tasks, where precise alignment of the face is necessary for accurate feature extraction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00ea2add",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import transform as trans\n",
    "\n",
    "def align_and_crop(img, landmarks, image_size=112):\n",
    "    \"\"\"\n",
    "    Align and crop the face from the image based on the given landmarks.\n",
    "\n",
    "    Args:\n",
    "        img (np.ndarray): The full image (not the cropped bounding box).\n",
    "        landmarks (List[np.ndarray]): List of 5 keypoints (landmarks) as (x, y) coordinates.\n",
    "        image_size (int, optional): The size to which the image should be resized. Defaults to 112.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, np.ndarray]: The aligned face image and the transformation matrix.\n",
    "    \"\"\"\n",
    "    _arcface_ref_kps = np.array(\n",
    "        [\n",
    "            [38.2946, 51.6963],\n",
    "            [73.5318, 51.5014],\n",
    "            [56.0252, 71.7366],\n",
    "            [41.5493, 92.3655],\n",
    "            [70.7299, 92.2041],\n",
    "        ],\n",
    "        dtype=np.float32,\n",
    "    )\n",
    "    assert len(landmarks) == 5\n",
    "    assert image_size % 112 == 0 or image_size % 128 == 0\n",
    "\n",
    "    if image_size % 112 == 0:\n",
    "        ratio = float(image_size) / 112.0\n",
    "        diff_x = 0\n",
    "    else:\n",
    "        ratio = float(image_size) / 128.0\n",
    "        diff_x = 8.0 * ratio\n",
    "\n",
    "    dst = _arcface_ref_kps * ratio\n",
    "    dst[:, 0] += diff_x\n",
    "    tform = trans.SimilarityTransform()\n",
    "    tform.estimate(np.array(landmarks), dst)\n",
    "    M = tform.params[0:2, :]\n",
    "\n",
    "    aligned_img = cv2.warpAffine(img, M, (image_size, image_size), borderValue=0.0)\n",
    "\n",
    "    return aligned_img, M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22af4704",
   "metadata": {},
   "source": [
    "#### Display the Aligned Faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6059cec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_faces=[]\n",
    "for idx, result in enumerate(face_det_kypts_inference_result.results):\n",
    "    landmarks = [landmark[\"landmark\"] for landmark in result[\"landmarks\"]]\n",
    "    aligned_face, _ = align_and_crop(image, landmarks)\n",
    "    aligned_faces.append(aligned_face)\n",
    "    \n",
    "# Display the concatenated aligned faces horizontally\n",
    "\n",
    "# cv2.imshow('Aligned Faces', cv2.hconcat(aligned_faces))\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n",
    "# print(\"Press 'x' or 'q' to stop.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3663925",
   "metadata": {},
   "source": [
    "#### Display both the Cropped faces and the Aligned faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36310019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 'x' or 'q' to stop.\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'cropped_faces' contains a list of cropped face images\n",
    "num_faces = len(cropped_faces)\n",
    "cols = 3  # Number of columns in the grid\n",
    "rows = (num_faces // cols) + (num_faces % cols > 0)  # Calculate rows needed\n",
    "\n",
    "# Calculate maximum width and height in one pass using zip to avoid multiple iterations\n",
    "max_width, max_height = max((img.shape[1], img.shape[0]) for img in cropped_faces)\n",
    "\n",
    "# Create a blank canvas to hold all the cropped faces in a grid layout\n",
    "canvas_width = cols * max_width\n",
    "canvas_height = rows * max_height\n",
    "canvas = np.zeros((canvas_height, canvas_width, 3), dtype=np.uint8)  # Blank canvas\n",
    "\n",
    "# Resize all cropped faces to match max width and height\n",
    "resized_faces = [cv2.resize(face, (max_width, max_height)) for face in cropped_faces]\n",
    "\n",
    "# Place each image in the correct location on the canvas (cropped faces grid)\n",
    "for idx, face in enumerate(resized_faces):\n",
    "    row = idx // cols  # Determine row number\n",
    "    col = idx % cols  # Determine column number\n",
    "\n",
    "    x_offset = col * max_width  # X position on canvas\n",
    "    y_offset = row * max_height  # Y position on canvas\n",
    "\n",
    "    # Place the face image on the canvas at the calculated position\n",
    "    canvas[y_offset:y_offset + face.shape[0], x_offset:x_offset + face.shape[1]] = face\n",
    "\n",
    "# Create the aligned faces (assuming 'align_and_crop' works properly)\n",
    "aligned_faces = []\n",
    "for idx, result in enumerate(face_det_kypts_inference_result.results):\n",
    "    landmarks = [landmark[\"landmark\"] for landmark in result[\"landmarks\"]]\n",
    "    aligned_face, _ = align_and_crop(image, landmarks)\n",
    "    aligned_faces.append(aligned_face)\n",
    "\n",
    "# Resize all aligned faces to match the max dimensions of the cropped faces (optional, for uniformity)\n",
    "resized_aligned_faces = [cv2.resize(face, (max_width, max_height)) for face in aligned_faces]\n",
    "\n",
    "# Concatenate all aligned faces horizontally (to make one row)\n",
    "aligned_faces_grid = cv2.hconcat(resized_aligned_faces)\n",
    "\n",
    "# Resize the cropped faces canvas to match the width of aligned faces grid\n",
    "canvas_resized = cv2.resize(canvas, (aligned_faces_grid.shape[1], canvas.shape[0]))\n",
    "\n",
    "# Check and ensure both grids have the same number of channels\n",
    "if canvas_resized.shape[2] != aligned_faces_grid.shape[2]:\n",
    "    canvas_resized = cv2.cvtColor(canvas_resized, cv2.COLOR_BGR2RGB)\n",
    "    aligned_faces_grid = cv2.cvtColor(aligned_faces_grid, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Stack the cropped faces grid on top of the aligned faces grid (vertical stack)\n",
    "final_display = cv2.vconcat([canvas_resized, aligned_faces_grid])\n",
    "\n",
    "# Show the final concatenated result\n",
    "cv2.imshow('Cropped and Aligned Faces', final_display)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(\"Press 'x' or 'q' to stop.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6fef46",
   "metadata": {},
   "source": [
    "**Similarity between two embeddings**\n",
    "\n",
    "In Face recognition, embeddings are numerical representations of a person's face. These embeddings capture the unique features of a face in a high-dimensional vector space, where faces that are similar will be closer together.\n",
    "\n",
    "To measure the similarity between two face embeddings, the most common approach is to calculate a distance metric between the two vectors. The closer the vectors are in space, the more similar the faces are.\n",
    "Two widely used metrics for this purpose are: \n",
    "1. Cosine Similarity  \n",
    "2. Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe0f3e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.07326925]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_reid_inference_result_cropped_face1 = face_reid_model(cropped_faces[0]) # Embedding for the first cropped face\n",
    "face_reid_inference_result_cropped_face2 = face_reid_model(cropped_faces[1]) # Embedding for the second cropped face\n",
    "\n",
    "# print (len(face_reid_inference_result_cropped_face1.results[0][\"data\"][0]))\n",
    "embedding1 = np.array(face_reid_inference_result_cropped_face1.results[0][\"data\"][0])\n",
    "embedding2 = np.array(face_reid_inference_result_cropped_face2.results[0][\"data\"][0])\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity([embedding1], [embedding2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33933181",
   "metadata": {},
   "source": [
    "## Database Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a8fc51",
   "metadata": {},
   "source": [
    "**Database indexing** is a technique used to improve the speed of data retrieval operations on a database table. \n",
    "Indexes allow for quick access to data without the need to scan every row, making data retrieval more efficient. \n",
    "In the context of modern vector databases like LanceDB, indexing is crucial for fast querying, especially when dealing with large datasets, such as embeddings for machine learning or image recognition tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf421ea",
   "metadata": {},
   "source": [
    "The components in database indexing are:\n",
    "\n",
    "    1. Image generator\n",
    "    2. Creating a new database table / Use the existing table\n",
    "    3. Database Schema\n",
    "    4. Adding data to the database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c884d21d",
   "metadata": {},
   "source": [
    "**Image generator** : This function is designed to iterate over a given directory or a single image file to generate paths to image files. Additionally, it can associate an entity name with each image based on its filename.\n",
    "\n",
    "Parameters:\n",
    "\n",
    "1. *input_path* (str or Path): This is the path to either a directory or a single image file. If a directory is provided, the function will recursively search for image files. If a single image file is provided, it will yield that file.\n",
    "\n",
    "2. *identity_name* (str, optional): An optional name to associate with the image. If not provided, the function will extract the name from the image file's name (based on splitting the filename by underscores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "410f2999",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def image_generator(input_path, identity_name = None):\n",
    "    \"\"\"Generate image paths from a given directory or a single image file.\"\"\"\n",
    "    path = Path(input_path)\n",
    "    # If the input path is a single file, yield it if it's an image along with its entity name\n",
    "    if path.is_file() and path.suffix.lower() in (\".png\", \".jpg\", \".jpeg\"):\n",
    "        entity_name = identity_name if identity_name is not None else path.stem.split(\"_\")[0]\n",
    "        yield str(path), {\"image_path\": str(path), \"entity_name\": entity_name}\n",
    "    # If it's a directory, yield all image files found within along with its entity name\n",
    "    else:\n",
    "        for file in path.rglob(\"*\"):\n",
    "            if file.suffix.lower() in (\".png\", \".jpg\", \".jpeg\"):\n",
    "                entity_name = file.stem.split(\"_\")[0]\n",
    "                yield str(file), {\"image_path\": str(file), \"entity_name\": entity_name}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6307c098",
   "metadata": {},
   "source": [
    "**Database Configuration**\n",
    "\n",
    "This section defines the key parameters for working with a database like the database **URI** is used to establish a connection to the database. \n",
    "The **table_name** is the new table/existing table within the database where embeddings, identities, and associated metadata are stored. Each entry in the table corresponds to a unique face and its corresponding data, and the **input_path** provides the location of the image data used to generate embeddings, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80cc4f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URI of the database where face data is stored. \n",
    "uri = \"face_database\"\n",
    "\n",
    "# Name of the table in the database that stores information such as facial embeddings, identities, and other related metadata.\n",
    "table_name = \"face\"\n",
    "\n",
    "# Path to the directory containing the sample dataset for indexing.\n",
    "input_path = \"../assets/Friends_dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce016f4f",
   "metadata": {},
   "source": [
    "The **FaceRecognitionSchema** in LanceDB defines the structure and data types for storing face recognition-related data, such as face embeddings, identities, and metadata. This schema is used to ensure consistency when storing and querying face recognition data within a LanceDB database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6dbf2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lancedb.pydantic import LanceModel, Vector\n",
    "import uuid\n",
    "\n",
    "# Define the Lance schema for face recognition\n",
    "class FaceRecognitionSchema(LanceModel):\n",
    "    id: str  # Unique identifier for each entry\n",
    "    vector: Vector(512)  # Face embeddings, fixed size of 512\n",
    "    image_path: str = \"image_path\"  # Default image path\n",
    "    entity_name: str = \"default\"  # Default entity name\n",
    "    bbox: Vector(4)  # Bounding box with 4 dimensions (x, y, width, height)\n",
    "    source: int = 0  # Source , default is 0\n",
    "\n",
    "    @classmethod\n",
    "    def format_data(cls, result) -> 'FaceRecognitionSchema':\n",
    "        \"\"\"Converts the result to a FaceRecognitionSchema instance.\n",
    "\n",
    "        Args:\n",
    "            result: A list of results containing embeddings and bounding box data.\n",
    "            image_path: The path to the image associated with the entries.\n",
    "            entity_name: Optional name for the entity; defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            A list of FaceRecognitionSchema instances.\n",
    "        \"\"\"\n",
    "        image_path, entity_name = result.info[\"image_path\"], result.info[\"entity_name\"]\n",
    "\n",
    "        data = [\n",
    "            cls(\n",
    "                id=str(uuid.uuid4()),  # Generate a unique ID for each entry\n",
    "                vector=np.array(res[\"embedding\"], dtype=np.float32),  # Convert embedding to a NumPy array with float32 dtype\n",
    "                image_path=image_path,  # Set the image path\n",
    "                entity_name=entity_name,  # Set the entity name, or use the default\n",
    "                bbox=np.array(res[\"bbox\"], dtype=np.float32)  # Convert bounding box to a NumPy array with float32 dtype\n",
    "            )\n",
    "            for res in result.results if \"embedding\" in res\n",
    "        ]\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fd82b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the configuration\n",
    "import lancedb\n",
    "# Connect to the LanceDB database\n",
    "db = lancedb.connect(uri=uri)\n",
    "\n",
    "# Check if the table exists, create if not\n",
    "if table_name not in db.table_names():\n",
    "    \"\"\"Create a new table in the database.\"\"\"\n",
    "    tbl = db.create_table(table_name, schema=FaceRecognitionSchema)\n",
    "else:\n",
    "    \"\"\"Open an existing table in the database.\"\"\"\n",
    "    tbl = db.open_table(table_name)\n",
    "    schema_fields = [field.name for field in tbl.schema]\n",
    "    if schema_fields != list(FaceRecognitionSchema.model_fields.keys()):\n",
    "        raise RuntimeError(\n",
    "            f\"Table {table_name} has a different schema.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49cfe53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added 19 entities to the face table.\n",
      "face table contains a total of 19 entities.\n"
     ]
    }
   ],
   "source": [
    "num_entities =  0 # Count the number of entities\n",
    "# Process images in batches\n",
    "for det_result in face_det_kypts_model.predict_batch(image_generator(input_path, identity_name=None)):\n",
    "    for result in det_result.results:\n",
    "        landmarks = [landmark[\"landmark\"] for landmark in result[\"landmarks\"]]\n",
    "        aligned_img, _ = align_and_crop(det_result.image, landmarks)\n",
    "        face_reid_inference_result = face_reid_model(aligned_img)\n",
    "        result[\"embedding\"] = face_reid_inference_result.results[0][\"data\"][0]\n",
    "        \n",
    "    # Format data for the FaceRecognitionSchema\n",
    "    data = FaceRecognitionSchema.format_data(det_result)\n",
    "    if len(data) > 0:\n",
    "        # Add the LanceSchema data to the table\n",
    "        tbl.add(data=data)\n",
    "    num_entities+=len(data)\n",
    "\n",
    "# Prints the number of entities added to the table\n",
    "print (f\"Successfully added {num_entities} entities to the {table_name} table.\")   \n",
    "# Prints the total number of entities in the table\n",
    "print(f\"{table_name} table contains a total of {tbl.count_rows()} entities.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c225a5a1",
   "metadata": {},
   "source": [
    "**Face Identification/Recognition**\n",
    "\n",
    "This typically involves comparing a face captured in an image or video against a database of known faces. The goal is to match the query face with one in the database to either identify the person or authenticate their identity.\n",
    "\n",
    "Search parameters: \n",
    "\n",
    "1. **Top-K** - The Top-K parameter defines the number of closest or most relevant results to return from a search query. In the context of face recognition, this often means retrieving the top K most similar face embeddings to a given query face embedding from the database.\n",
    "2. **Field_name** - Field_name refers to the specific field or column within the database that will be searched. This could refer to attributes like facial embeddings, identity names, timestamps, or other metadata associated with the faces in the database.\n",
    "3. **Metric type** - The Metric Type defines the similarity measure used to compare the face embeddings in the database during the search process. It is a critical parameter for determining how the system calculates the \"closeness\" or \"similarity\" between faces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb1a8155",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 1\n",
    "field_name = \"vector\"\n",
    "metric_type = \"cosine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36c29213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_face_result(tbl, result, field_name, metric_type, top_k, threshold=0.3):\n",
    "    \"\"\"Process the face result: perform database search, calculate distance, and assign label.\"\"\"\n",
    "    for i, res in enumerate(result.results):\n",
    "        # Perform database search\n",
    "        search_result = (\n",
    "            tbl.search(\n",
    "                np.array(res[\"embedding\"]).astype(np.float32),\n",
    "                vector_column_name=field_name,\n",
    "            )\n",
    "            .metric(metric_type)\n",
    "            .limit(top_k)\n",
    "            .to_list()\n",
    "        )\n",
    "\n",
    "        # Calculate distance and assign label\n",
    "        distance = round(1 - search_result[0][\"_distance\"], 2)\n",
    "        if distance >= threshold:\n",
    "            res[\"label\"] = search_result[0][\"entity_name\"]\n",
    "        else:\n",
    "            res[\"label\"] = \"Unknown\"\n",
    "\n",
    "        res[\"score\"] = distance\n",
    "\n",
    "        # Clean up unnecessary fields\n",
    "        result.results[i].pop(\"landmarks\", None)\n",
    "        result.results[i].pop(\"embedding\", None)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1cb94d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# db = lancedb.connect(uri=uri)\n",
    "if table_name in db.table_names():\n",
    "    tbl = db.open_table(table_name)\n",
    "    schema_fields = [field.name for field in tbl.schema]\n",
    "    if schema_fields != list(FaceRecognitionSchema.model_fields.keys()):\n",
    "        raise RuntimeError(f\"Table {table_name} has a different schema.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "122c767e",
   "metadata": {},
   "outputs": [],
   "source": [
    "det_result = face_det_kypts_model.predict(image_source) \n",
    "image = det_result.image\n",
    "for result in det_result.results:\n",
    "    landmarks = [landmark[\"landmark\"] for landmark in result[\"landmarks\"]]\n",
    "    aligned_img, _ = align_and_crop(det_result.image, landmarks)\n",
    "    face_reid_inference_result = face_reid_model(aligned_img)\n",
    "    result[\"embedding\"] = face_reid_inference_result.results[0][\"data\"][0]\n",
    "    \n",
    "search_result = process_face_result(tbl, det_result, field_name, metric_type, top_k)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7252261d",
   "metadata": {},
   "source": [
    "## Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb1c8e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from degirum_tools.ui_support import Display\n",
    "win_name = f\"Annotated Image\"\n",
    "display = Display(win_name)\n",
    "img = search_result.image_overlay\n",
    "display.show(img)\n",
    "# Wait for the user to press a key\n",
    "while True:\n",
    "    key = cv2.waitKey(1) & 0xFF  # Wait for key press\n",
    "    if key == ord('x') or key == ord('q'):\n",
    "        break  # Exit the loop if 'x' or 'q' is pressed\n",
    "\n",
    "cv2.destroyAllWindows()  # Close the window"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
