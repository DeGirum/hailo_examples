{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2a93c3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import degirum as dg, degirum_tools\n",
    "import cv2, numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "6091ed2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose inference host address\n",
    "inference_host_address = \"@cloud\" \n",
    "# inference_host_address = \"@local\"\n",
    "\n",
    "# choose zoo_url\n",
    "zoo_url = \"degirum/models_hailort\"\n",
    "# zoo_url = \"<path to local folder>\"\n",
    "\n",
    "# set token\n",
    "token = degirum_tools.get_token()\n",
    "# token = '' # leave empty for local inference\n",
    "\n",
    "# choose image source\n",
    "image_source = \"../assets/Friends.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638f4cf1",
   "metadata": {},
   "source": [
    "**Face Detection and Keypoint Estimation with DeGirum PySDK** \n",
    "\n",
    "It loads a YOLOv8 face detection model with keypoint estimation and applies it to an input image. The detected faces and their corresponding keypoints are displayed visually, providing insights into facial regions. This setup is ideal for tasks such as face analysis, landmark detection, or pose estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27393e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_det_kypts_model_name = \"yolov8n_relu6_widerface_kpts--640x640_quant_hailort_hailo8l_1\"\n",
    "\n",
    "# load AI model\n",
    "face_det_kypts_model = dg.load_model(\n",
    "    model_name=face_det_kypts_model_name,\n",
    "    inference_host_address=inference_host_address,\n",
    "    zoo_url=zoo_url,\n",
    "    token=token\n",
    ")\n",
    "\n",
    "# perform AI model inference on given image source\n",
    "print(f\" Running inference using '{face_det_kypts_model_name}' on image source '{image_source}'\")\n",
    "face_det_kypts_inference_result = face_det_kypts_model(image_source)\n",
    "\n",
    "print('Inference Results \\n', face_det_kypts_inference_result)  # Detection Results with keypoints\n",
    "\n",
    "# Show results of inference\n",
    "with degirum_tools.Display(\"AI Camera\") as output_display:\n",
    "    output_display.show_image(face_det_kypts_inference_result)\n",
    "    \n",
    "print(\"Press 'x' or 'q' to stop.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9328d58a",
   "metadata": {},
   "source": [
    "**Face Re-identification with DeGirum PySDK**\n",
    "\n",
    "This script loads a YOLOv8-based face re-identification (ReID) model and applies it to the face crops obtained from YOLOv8 face detection model with keypoint estimation. The model extracts extracts unique features/embeddings from each cropped face.\n",
    "\n",
    "The output of the ReID model is a 512-dimensional embedding, which encapsulates the unique features of each cropped face, enabling precise identification and comparison across different images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943eb22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_reid_model_name = \"arcface_mobilefacenet--112x112_quant_hailort_hailo8_2\"\n",
    "zoo_url = \"degirum/sandbox_shashi\"\n",
    "\n",
    "# load AI model\n",
    "face_reid_model = dg.load_model(\n",
    "    model_name=face_reid_model_name,\n",
    "    inference_host_address=inference_host_address,\n",
    "    zoo_url=zoo_url,\n",
    "    token=token\n",
    ")\n",
    "image = face_det_kypts_inference_result.image\n",
    "\n",
    "cropped_faces=[]\n",
    "for face in face_det_kypts_inference_result.results:\n",
    "    x1, y1, x2, y2 = map(int, face[\"bbox\"])\n",
    "    # Crop the face from the image using corner coordinates\n",
    "    cropped_face = image[y1:y2, x1:x2] # for opencv\n",
    "    # cropped_face = image.crop((x1, y1, x2, y2)) # for pil\n",
    "    # Append the cropped face to the list\n",
    "    cropped_faces.append(cropped_face)\n",
    "\n",
    "# Display all cropped faces\n",
    "for idx, cropped_face in enumerate(cropped_faces):\n",
    "    # Display each cropped face using OpenCV\n",
    "    cv2.imshow(f\"Cropped Face {idx + 1}\", cropped_face)\n",
    "\n",
    "cv2.waitKey(0)  # Wait for a key press to close the windows\n",
    "cv2.destroyAllWindows()  # Close all OpenCV windows\n",
    "\n",
    "print(f\" Running inference using '{face_reid_model_name}' on cropped face\")\n",
    "face_reid_inference_result = face_reid_model(cropped_faces[0]) # Embedding for a single cropped face\n",
    "print ('Shape of a face embedding:', len(face_reid_inference_result.results[0][\"data\"][0])) # Print the length of the embedding for a single face."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f3259b",
   "metadata": {},
   "source": [
    "**Align and Crop**\n",
    "\n",
    "The `align_and_crop` function is designed to align and crop a face from an image based on a given set of landmarks. This is particularly useful in facial recognition tasks, where precise alignment of the face is necessary for accurate feature extraction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "00ea2add",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import transform as trans\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def align_and_crop(img, landmarks, image_size=112):\n",
    "    \"\"\"\n",
    "    Align and crop the face from the image based on the given landmarks.\n",
    "\n",
    "    Args:\n",
    "        img (np.ndarray): The full image (not the cropped bounding box). This image will be transformed.\n",
    "        landmarks (List[np.ndarray]): List of 5 keypoints (landmarks) as (x, y) coordinates. These keypoints typically include the eyes, nose, and mouth.\n",
    "        image_size (int, optional): The size to which the image should be resized. Defaults to 112. It is typically either 112 or 128 for face recognition models.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, np.ndarray]: The aligned face image and the transformation matrix.\n",
    "    \"\"\"\n",
    "    # Define the reference keypoints used in ArcFace model, based on a typical facial landmark set.\n",
    "    _arcface_ref_kps = np.array(\n",
    "        [\n",
    "            [38.2946, 51.6963],  # Left eye\n",
    "            [73.5318, 51.5014],  # Right eye\n",
    "            [56.0252, 71.7366],  # Nose\n",
    "            [41.5493, 92.3655],  # Left mouth corner\n",
    "            [70.7299, 92.2041],  # Right mouth corner\n",
    "        ],\n",
    "        dtype=np.float32,\n",
    "    )\n",
    "\n",
    "    # Ensure the input landmarks have exactly 5 points (as expected for face alignment)\n",
    "    assert len(landmarks) == 5\n",
    "\n",
    "    # Validate that image_size is divisible by either 112 or 128 (common image sizes for face recognition models)\n",
    "    assert image_size % 112 == 0 or image_size % 128 == 0\n",
    "\n",
    "    # Adjust the scaling factor (ratio) based on the desired image size (112 or 128)\n",
    "    if image_size % 112 == 0:\n",
    "        ratio = float(image_size) / 112.0\n",
    "        diff_x = 0  # No horizontal shift for 112 scaling\n",
    "    else:\n",
    "        ratio = float(image_size) / 128.0\n",
    "        diff_x = 8.0 * ratio  # Horizontal shift for 128 scaling\n",
    "\n",
    "    # Apply the scaling and shifting to the reference keypoints\n",
    "    dst = _arcface_ref_kps * ratio\n",
    "    dst[:, 0] += diff_x  # Apply the horizontal shift\n",
    "\n",
    "    # Estimate the similarity transformation matrix to align the landmarks with the reference keypoints\n",
    "    tform = trans.SimilarityTransform()\n",
    "    tform.estimate(np.array(landmarks), dst)\n",
    "    \n",
    "    # Extract the affine transformation matrix (2x3) from the transformation object\n",
    "    M = tform.params[0:2, :]  # This is a 2x3 matrix for the affine transformation\n",
    "\n",
    "    # Apply the affine transformation to the input image to align the face\n",
    "    aligned_img = cv2.warpAffine(img, M, (image_size, image_size), borderValue=0.0)\n",
    "\n",
    "    return aligned_img, M\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22af4704",
   "metadata": {},
   "source": [
    "#### Display the Aligned Faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6059cec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_faces=[]\n",
    "\n",
    "for idx, result in enumerate(face_det_kypts_inference_result.results):\n",
    "    landmarks = [landmark[\"landmark\"] for landmark in result[\"landmarks\"]]\n",
    "    aligned_face, _ = align_and_crop(image, landmarks)\n",
    "    aligned_faces.append(aligned_face)\n",
    "        \n",
    "# Display the concatenated aligned faces horizontally\n",
    "cv2.imshow('Aligned Faces', cv2.hconcat(aligned_faces))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "print(\"Press 'x' or 'q' to stop.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6fef46",
   "metadata": {},
   "source": [
    "**Similarity between two embeddings**\n",
    "\n",
    "In Face recognition, embeddings are numerical representations of a person's face. These embeddings capture the unique features of a face in a high-dimensional vector space, where faces that are similar will be closer together.\n",
    "\n",
    "To measure the similarity between two face embeddings, the most common approach is to calculate a distance metric between the two vectors. The closer the vectors are in space, the more similar the faces are.\n",
    "Two widely used metrics for this purpose are: \n",
    "1. Cosine Similarity  \n",
    "2. Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0f3e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider two different faces \n",
    "input_image1 = \"../assets/Friends_dataset/Chandler_1.jpg\"\n",
    "input_image2 = \"../assets/Friends_dataset/Joey_1.jpg\"\n",
    "\n",
    "# Run Face detection and keypoints model on the first input image (Chandler_1.jpg)\n",
    "face_det_kypts_inference_result1 = face_det_kypts_model(input_image1)\n",
    "\n",
    "# Extract the facial landmarks for alignment\n",
    "landmarks = [landmark[\"landmark\"] for landmark in face_det_kypts_inference_result1.results[0][\"landmarks\"]]\n",
    "aligned_img, _ = align_and_crop(face_det_kypts_inference_result1.image, landmarks)  # Align and crop the face\n",
    "\n",
    "# Generate the face embedding using the face re-identification model\n",
    "face_reid_inference_result1 = face_reid_model(aligned_img)\n",
    "embedding1 =  np.array(face_reid_inference_result1.results[0][\"data\"][0])\n",
    "\n",
    "\n",
    "# Run Face detection and keypoints model on second input image (Joey_1.jpg)\n",
    "face_det_kypts_inference_result2 = face_det_kypts_model(input_image2)\n",
    "\n",
    "# Extract the facial landmarks for alignment\n",
    "landmarks = [landmark[\"landmark\"] for landmark in face_det_kypts_inference_result2.results[0][\"landmarks\"]]\n",
    "aligned_img, _ = align_and_crop(face_det_kypts_inference_result2.image, landmarks)  # Align and crop the face\n",
    "\n",
    "# Generate the face embedding using the face re-identification model\n",
    "face_reid_inference_result2 = face_reid_model(aligned_img)\n",
    "embedding2 =  np.array(face_reid_inference_result2.results[0][\"data\"][0])\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity([embedding1], [embedding2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824f86c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider two same faces\n",
    "input_image1 = \"../assets/Friends_dataset/Joey_1.jpg\"\n",
    "input_image2 = \"../assets/Friends_dataset/Joey_2.jpg\"\n",
    "\n",
    "# Run Face detection and keypoints model on the first input image (Joey_1.jpg)\n",
    "face_det_kypts_inference_result1 = face_det_kypts_model(input_image1)\n",
    "\n",
    "# Extract the facial landmarks for alignment\n",
    "landmarks = [landmark[\"landmark\"] for landmark in face_det_kypts_inference_result1.results[0][\"landmarks\"]]\n",
    "aligned_img, _ = align_and_crop(face_det_kypts_inference_result1.image, landmarks)  # Align and crop the face\n",
    "\n",
    "# Generate the face embedding using the face re-identification model\n",
    "face_reid_inference_result1 = face_reid_model(aligned_img)\n",
    "embedding1 =  np.array(face_reid_inference_result1.results[0][\"data\"][0])\n",
    "\n",
    "\n",
    "# Run Face detection and keypoints model on second input image (Joey_2.jpg)\n",
    "face_det_kypts_inference_result2 = face_det_kypts_model(input_image2)\n",
    "\n",
    "# Extract the facial landmarks for alignment\n",
    "landmarks = [landmark[\"landmark\"] for landmark in face_det_kypts_inference_result2.results[0][\"landmarks\"]]\n",
    "aligned_img, _ = align_and_crop(face_det_kypts_inference_result2.image, landmarks)  # Align and crop the face\n",
    "\n",
    "# Generate the face embedding using the face re-identification model\n",
    "face_reid_inference_result2 = face_reid_model(aligned_img)\n",
    "embedding2 =  np.array(face_reid_inference_result2.results[0][\"data\"][0])\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity([embedding1], [embedding2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6deeac2",
   "metadata": {},
   "source": [
    "In the above similarity calculation, the two faces that are similar will produce a higher similarity score, indicating a closer match, while faces that are distinct will yield a lower similarity score, reflecting the greater dissimilarity between them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33933181",
   "metadata": {},
   "source": [
    "## Database Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a8fc51",
   "metadata": {},
   "source": [
    "**Database indexing** is a technique used to improve the speed of data retrieval operations on a database table. \n",
    "Indexes allow for quick access to data without the need to scan every row, making data retrieval more efficient. \n",
    "In the context of modern vector databases like LanceDB, indexing is crucial for fast querying, especially when dealing with large datasets, such as embeddings for machine learning or image recognition tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf421ea",
   "metadata": {},
   "source": [
    "The components in database indexing are:\n",
    "\n",
    "    1. Creating a new database table / Use the existing table\n",
    "    2. Database Schema\n",
    "    3. Adding data to the database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6307c098",
   "metadata": {},
   "source": [
    "**Database Configuration**\n",
    "\n",
    "This section defines the key parameters for working with a database like the database **URI** is used to establish a connection to the database. \n",
    "The **table_name** is the new table/existing table within the database where embeddings, identities, and associated metadata are stored. Each entry in the table corresponds to a unique face and its corresponding data, and the **input_path** provides the location of the image data used to generate embeddings, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "80cc4f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URI of the database where face data is stored. \n",
    "uri = \"face_database\"\n",
    "\n",
    "# Name of the table in the database that stores information such as facial embeddings, identities, and other related metadata.\n",
    "table_name = \"face\"\n",
    "\n",
    "# Path to the directory containing the sample dataset for indexing.\n",
    "input_path = \"../assets/Friends_dataset\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce016f4f",
   "metadata": {},
   "source": [
    "The **FaceRecognitionSchema** in LanceDB defines the structure and data types for storing face recognition-related data, such as face embeddings, identities, and metadata. This schema is used to ensure consistency when storing and querying face recognition data within a LanceDB database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d6dbf2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lancedb.pydantic import LanceModel, Vector\n",
    "import uuid\n",
    "\n",
    "# Define the Lance schema for face recognition\n",
    "class FaceRecognitionSchema(LanceModel):\n",
    "    id: str  # Unique identifier for each entry\n",
    "    vector: Vector(512)  # Face embeddings, fixed size of 512\n",
    "    image_path: str = \"image_path\"  # Default image path\n",
    "    entity_name: str = \"default\"  # Default entity name\n",
    "    bbox: Vector(4)  # Bounding box with 4 dimensions (x, y, width, height)\n",
    "    source: int = 0  # Source , default is 0\n",
    "\n",
    "    @classmethod\n",
    "    def format_data(cls, result, image_path, entity_name) -> 'FaceRecognitionSchema':\n",
    "        \"\"\"Converts the result to a FaceRecognitionSchema instance.\n",
    "\n",
    "        Args:\n",
    "            result: A list of results containing embeddings and bounding box data.\n",
    "            image_path: The path to the image associated with the entries.\n",
    "            entity_name: Optional name for the entity; defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            A list of FaceRecognitionSchema instances.\n",
    "        \"\"\"\n",
    "\n",
    "        data = [\n",
    "            cls(\n",
    "                id=str(uuid.uuid4()),  # Generate a unique ID for each entry\n",
    "                vector=np.array(res[\"embedding\"], dtype=np.float32),  # Convert embedding to a NumPy array with float32 dtype\n",
    "                image_path=image_path,  # Set the image path\n",
    "                entity_name=entity_name,  # Set the entity name, or use the default\n",
    "                bbox=np.array(res[\"bbox\"], dtype=np.float32)  # Convert bounding box to a NumPy array with float32 dtype\n",
    "            )\n",
    "            for res in result.results if \"embedding\" in res\n",
    "        ]\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "0fd82b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the configuration\n",
    "import lancedb\n",
    "# Connect to the LanceDB database\n",
    "db = lancedb.connect(uri=uri)\n",
    "\n",
    "# Check if the table exists, create if not\n",
    "if table_name not in db.table_names():\n",
    "    \"\"\"Create a new table in the database.\"\"\"\n",
    "    tbl = db.create_table(table_name, schema=FaceRecognitionSchema)\n",
    "else:\n",
    "    \"\"\"Open an existing table in the database.\"\"\"\n",
    "    tbl = db.open_table(table_name)\n",
    "    schema_fields = [field.name for field in tbl.schema]\n",
    "    if schema_fields != list(FaceRecognitionSchema.model_fields.keys()):\n",
    "        raise RuntimeError(\n",
    "            f\"Table {table_name} has a different schema.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbefd71",
   "metadata": {},
   "source": [
    "The following code processes face images stored in a specified directory and applies face detection using face detection+keypoints model, cropping and alignment using the 5 facial landmarks, and feature extraction using the face-reid models. The extracted features are then stored in a LanceDB table with a specific schema for future retrieval and comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1af679e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "path = Path(input_path)\n",
    "num_entities = 0  # Variable to keep track of the number of entities added to the database\n",
    "# Iterate over all files in the directory (and subdirectories)\n",
    "for file in path.rglob(\"*\"):\n",
    "    # Check if the file is an image (based on extension)\n",
    "    if file.suffix.lower() in (\".png\", \".jpg\", \".jpeg\"):\n",
    "        entity_name = file.stem.split(\"_\")[0]  # Extract entity name from the file name (first part before \"_\")\n",
    "        image_path = str(file)  # Get the path to the image as a string\n",
    "        \n",
    "        # Perform face detection and keypoint extraction using the face detection and keypoints model\n",
    "        face_det_kypts_inference_result = face_det_kypts_model(image_path)\n",
    "        \n",
    "        # Iterate through the detected faces in the image\n",
    "        for result in face_det_kypts_inference_result.results:\n",
    "            # Extract the facial landmarks for alignment\n",
    "            landmarks = [landmark[\"landmark\"] for landmark in result[\"landmarks\"]]\n",
    "            aligned_img, _ = align_and_crop(face_det_kypts_inference_result.image, landmarks)  # Align and crop the face\n",
    "            \n",
    "            # Generate the face embedding using the face re-identification model\n",
    "            face_reid_inference_result = face_reid_model(aligned_img)\n",
    "            result[\"embedding\"] = face_reid_inference_result.results[0][\"data\"][0]  # Store the generated embedding\n",
    "            \n",
    "        # Format the extracted data (face metadata and embeddings) for storage in the database\n",
    "        data = FaceRecognitionSchema.format_data(face_det_kypts_inference_result, image_path, entity_name)\n",
    "        \n",
    "        # Check if the formatted data is valid (non-empty)\n",
    "        if len(data) > 0:\n",
    "            # Add the valid data to the database table\n",
    "            tbl.add(data=data)\n",
    "        \n",
    "        # Increment the number of entities added to the database\n",
    "        num_entities += len(data)\n",
    "\n",
    "print(f\"Successfully added {num_entities} entities to the {table_name} table.\")  # Print number of entities added\n",
    "print(f\"{table_name} table contains a total of {tbl.count_rows()} entities.\")  # Print total number of entities in the table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c225a5a1",
   "metadata": {},
   "source": [
    "**Face Identification/Recognition**\n",
    "\n",
    "This typically involves comparing a face captured in an image or video against a database of known faces. The goal is to match the query face with one in the database to either identify the person or authenticate their identity.\n",
    "\n",
    "Search parameters: \n",
    "\n",
    "1. **Top-K** - The Top-K parameter defines the number of closest or most relevant results to return from a search query. In the context of face recognition, this often means retrieving the top K most similar face embeddings to a given query face embedding from the database.\n",
    "2. **Field_name** - Field_name refers to the specific field or column within the database that will be searched. This could refer to attributes like facial embeddings, identity names, timestamps, or other metadata associated with the faces in the database.\n",
    "3. **Metric type** - The Metric Type defines the similarity measure used to compare the face embeddings in the database during the search process. It is a critical parameter for determining how the system calculates the \"closeness\" or \"similarity\" between faces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "bb1a8155",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 1\n",
    "field_name = \"vector\"\n",
    "metric_type = \"cosine\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892854e3",
   "metadata": {},
   "source": [
    "The **process_face_result** function processes a set of face detection results along with the extracted embeddings, searches for the most similar faces in a database, calculates the similarity score (distance), and assigns labels to each face based on the similarity threshold. \n",
    "\n",
    "If the calculated similarity score (distance) exceeds a predefined threshold, the detected face is labeled with the identity from the database. Otherwise, the face is labeled as \"Unknown\". This threshold allows for rejecting faces with low similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "36c29213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_face_result(tbl, result, field_name, metric_type, top_k, threshold=0.3):\n",
    "    \n",
    "    \"\"\"Process the face result: perform database search, calculate distance, and assign label.\"\"\"\n",
    "    for i, res in enumerate(result.results):\n",
    "        # Perform database search\n",
    "        search_result = (\n",
    "            tbl.search(\n",
    "                np.array(res[\"embedding\"]).astype(np.float32),\n",
    "                vector_column_name=field_name,  # Specify the name of the vector column in the database\n",
    "            )\n",
    "            .metric(metric_type)\n",
    "            .limit(top_k)\n",
    "            .to_list()\n",
    "        )\n",
    "\n",
    "        # Calculate the distance\n",
    "        distance = round(1 - search_result[0][\"_distance\"], 2)\n",
    "\n",
    "        # Assign a label based on the distance threshold\n",
    "        if distance >= threshold:\n",
    "            res[\"label\"] = search_result[0][\"entity_name\"]\n",
    "        else:\n",
    "            res[\"label\"] = \"Unknown\"\n",
    "\n",
    "        res[\"score\"] = distance  # Store the calculated similarity score\n",
    "\n",
    "        # Clean up unnecessary fields\n",
    "        result.results[i].pop(\"landmarks\", None)\n",
    "        result.results[i].pop(\"embedding\", None)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "1cb94d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# db = lancedb.connect(uri=uri)\n",
    "if table_name in db.table_names():\n",
    "    tbl = db.open_table(table_name)\n",
    "    schema_fields = [field.name for field in tbl.schema]\n",
    "    if schema_fields != list(FaceRecognitionSchema.model_fields.keys()):\n",
    "        raise RuntimeError(f\"Table {table_name} has a different schema.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6073a693",
   "metadata": {},
   "source": [
    "The following code takes an input image (query image), processes it to extract facial features, and then performs face recognition by comparing the extracted features with stored facial embeddings in a database (LanceDB).\n",
    "The results are returned as \"search_result\", which contains the recognized face's label and the calculated similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "122c767e",
   "metadata": {},
   "outputs": [],
   "source": [
    "det_result = face_det_kypts_model(image_source) \n",
    "image = det_result.image\n",
    "for result in det_result.results:\n",
    "    landmarks = [landmark[\"landmark\"] for landmark in result[\"landmarks\"]]\n",
    "    aligned_img, _ = align_and_crop(det_result.image, landmarks)\n",
    "    face_reid_inference_result = face_reid_model(aligned_img)\n",
    "    result[\"embedding\"] = face_reid_inference_result.results[0][\"data\"][0]\n",
    "    \n",
    "search_result = process_face_result(tbl, det_result, field_name, metric_type, top_k)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7252261d",
   "metadata": {},
   "source": [
    "## Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "eb1c8e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from degirum_tools.ui_support import Display\n",
    "win_name = f\"Annotated Image\"\n",
    "display = Display(win_name)\n",
    "img = search_result.image_overlay\n",
    "display.show(img)\n",
    "# Wait for the user to press a key\n",
    "while True:\n",
    "    key = cv2.waitKey(1) & 0xFF  # Wait for key press\n",
    "    if key == ord('x') or key == ord('q'):\n",
    "        break  # Exit the loop if 'x' or 'q' is pressed\n",
    "\n",
    "cv2.destroyAllWindows()  # Close the window"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
